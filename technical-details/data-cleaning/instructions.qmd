**Introduction**
The purpose of this data cleaning page was to take the JSON files created in the data-collecting step, combine and flatten the data into a singular Pandas data frame. Data cleaning was an interative process I needed to add and alter throughout the Project. Once the Pandas dataframe was build, i needed to remove parts of the dataset, and create additional calculated fields that would be used in the EDA, supervised, and unsupervised learning sections.This final data frame was then saved as a csv file for both documentation and later use in the EDA step. 


**Process**
The raw JSON file consists of multi-level nested dictionaries and lists. To flatten these into a dataframe, I used Python.

Once the JSON files had been combined, the shape was (50825, 23). 8662 unique player IDs across 6388 game were captured which is still robust despite my limitations.

I first looked at the data types.
Data types were object, int64, and float 64. 
Time fields game_length and time_eliminated being float was acceptable as time was measured in seconds.

Missing data was identified using this code line:
dfRaw.notnull().mean() * 100 
Which provided how many of a fields data was null. 

My result was 100% except for the field augments which was 0. As augments had no data, it was removed.
This was likely due an error on the data-collection process. I was unable to resolve this due to the Riot API having no data to pull when I reran due to the release of Build 16. The rest of my fields had no null values.

Outlier Detection and Treatment and Normalization and Scaling were not performed at this time. I wanted my final cleaned dataset to not alter the data that exists. Instead, looking at outliers, normalization, scaling, skew and kurtosis are evaluated in the EDA section, and then performed during the modeling section. 

The final dataset was saved as csv file for usage in EDA, unsupervised and supervised learning sections.

- **Visualizations**: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.
By the end of this phase, your cleaned data should be well-documented and ready for further stages, such as **Exploratory Data Analysis (EDA)** and **Machine Learning**.

## What to address 

The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.

The **Data Cleaning** page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.

The following is a guide to help you get started with possible thing to address on this page .

- **Description of the Data Cleaning Process**: Explain the steps you took to clean and preprocess the data.
- **Code Documentation**: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).
- **Provide examples of data before and after cleaning**: e.g. with df.head() or df.describe()
- **Raw and Cleaned Data Links**: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in `data/processed-data`, or similar location which doesn't get synced to GitHub)

Possible things to include:

**Introduction to Data Cleaning**:

- Provide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.
- Mention that data cleaning may need to be revisited as the project evolves and analysis goals change.

**Managing Missing Data**:

- **Identify Missing Values**: Explain how you identified missing data and where it occurred.
- **Handling Missing Data**: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).
- **Visualize Missing Data**: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.

**Outlier Detection and Treatment**:

- **Identify Outliers**: Describe the methods you used to detect outliers in the dataset.
- **Addressing Outliers**: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).
- **Visualize Outliers**: Use visualizations (e.g., box plots) to show how outliers were managed.


**Data Type Correction and Formatting**:

- **Review Data Types**: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.
- **Transformation**: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.
- **Impact of Changes**: Briefly explain why these changes were necessary for accurate analysis.

**Normalization and Scaling**:

- **Data Distribution Analysis**: Check and discuss the distribution of numerical variables (e.g., skewness).
- **Normalization Techniques**: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).
- **Before-and-After Visualizations**: Provide visualizations comparing the data before and after scaling or normalization.

**Subsetting the Data**:

- **Data Filtering**: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).
- **Rationale**: Justify why you chose to work with a particular subset of the data.


