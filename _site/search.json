[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Introduction:\nThis project explores unsupervised learning techniques to identify common meta compositions (“comps”) used by top-ranked players in Teamfight Tactics (TFT), Set 15. The primary research questions guiding this analysis are:\nWhat team composition is the most frequently used by top-level TFT players? Which composition places the highest at top-level play?\nIn TFT, champions have unique abilities and share traits that activate synergies when fielded together. For example, Ahri and Jinx share the Star Guardian trait, granting increased bonuses when multiple Star Guardians appear in the team. Understanding which champion combinations appear consistently in successful boards may reveal the meta trends that drive competitive play.\n\n\nMethods\nI had used my cleaned dataset to create a new dataset consisting of 32,455 rows and 63 champion indicator columns, where each column represented the frequency or presence of a champion in the player’s final board. Because analyzing every individual game state introduces high redundancy, duplicate rows were removed, producing a dataset of 10,757 unique compositions.\nThis reduction is expected because high-level players tend to converge on optimal meta comps despite the game’s inherent randomness (RNG)\nInitial Attempts Using HDBSCAN\nMy initial attemp was to use use HDBSCAN, which is well-suited for clusters of varying densities. However, it was not producing ideal results. As the data is purely categorical, Euclidean distance is unsuitable. Switching to Jaccard distance produced unstable clusters with poor separation.\nDue to these limitations, I had decided to apply hierarchial clustering instead. Like HDBSCAN, it was initially run using Jaccard distance, but the number of clusters exceeded 100, making interpretation difficult. To address this, Principal Component Analysis (PCA) was applied prior to clustering. PCA was applied with n_components = 25 and the retained components explained 82.6% of the variance. After PCA was applied, clustering transitioned to using Euclidean distance.\nAfter creating a dendrogram, a threshold parameter (t) was evaluated to determine the cut point in the dendrogram. This would determine the number of clusters. The results are shown below:\nt = 2 → clusters = 228 t = 3 → clusters = 11 t = 4 → clusters = 1 t = 5 → clusters = 1\nBased on cluster separation and number of clusters, t = 3 was selected, producing 11 meaningful clusters representing meta team compositions.\n#Methods Hierarchic Clustering: - A clustering algorithm.\nHDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) - HDBSCAN is a clustering algorithm build upon DBSCAN. It can dynamically adapt adjust to different densities and form clusters within the data. It is best used for datasets with complex structures or varying densities because it creates a hierarchical tree of clusters that enable users to examine the data at different levels of granularity. Parameters that can be adjusted by the ende user are: The minimum number of points to form a cluster. The minimum number of samples in a neighborhood for a point to be considered a core point.\nPrincipal Component Analysis (PCA): -\nTSNE -\n#Results Section: Below is a table showing the cluster and the number of points in each cluster.\n3 1680 6 1505 5 1363 10 960 1 955 4 852 2 819 9 802 11 744 7 566 8 511\nClusters 3,6, and 5 are the largest.\nBelow are the 11 clusters produced. The values represent frequency of champion appearance within the cluster. A higher value, indicates if they are a staple,secondary, or niche champion pick for each team comp.\n===== COMP 1 ===== Poppy: 0.95 Jinx: 0.95 Neeko: 0.81 Rell: 0.75 Kobuko: 0.72 KSante: 0.68 Xayah: 0.52 Seraphine: 0.48 Varus: 0.47 Braum: 0.46\n===== COMP 2 ===== Garen: 0.95 Rakan: 0.90 Ezreal: 0.88 Leona: 0.84 Yuumi: 0.83 Jayce: 0.57 Katarina: 0.53 Ryze: 0.31 Malzahar: 0.29 KSante: 0.29\n===== COMP 3 ===== Shen: 0.89 Leona: 0.85 Rell: 0.84 Swain: 0.83 XinZhao: 0.81 Braum: 0.66 Garen: 0.43 Seraphine: 0.38 TwistedFate: 0.37 Zyra: 0.31\n===== COMP 4 ===== Samira: 0.93 Sett: 0.92 Volibear: 0.77 Naafiri: 0.73 XinZhao: 0.72 Lux: 0.54 Gwen: 0.48 Braum: 0.46 Viego: 0.40 Shen: 0.38\n===== COMP 5 ===== DrMundo: 0.93 Udyr: 0.91 Sett: 0.87 Naafiri: 0.80 Vi: 0.71 LeeSin: 0.71 Braum: 0.54 TwistedFate: 0.53 Zyra: 0.45 Aatrox: 0.44\n===== COMP 6 ===== Swain: 0.91 Janna: 0.90 Ashe: 0.85 Vi: 0.77 Zyra: 0.71 KSante: 0.69 JarvanIV: 0.67 Braum: 0.55 Udyr: 0.53 LeeSin: 0.47\n===== COMP 7 ===== Janna: 0.83 Malphite: 0.81 Sivir: 0.60 Ziggs: 0.57 Shen: 0.57 Ryze: 0.46 JarvanIV: 0.44 Neeko: 0.41 KSante: 0.34 Kennen: 0.31\n===== COMP 8 ===== Karma: 0.97 Lucian: 0.94 JarvanIV: 0.90 Ryze: 0.66 Swain: 0.51 Aatrox: 0.49 Senna: 0.46 Gwen: 0.44 Lux: 0.43 Gangplank: 0.41\n===== COMP 9 ===== Kayle: 0.82 Udyr: 0.79 Zac: 0.72 Jhin: 0.62 Aatrox: 0.60 Varus: 0.54 KSante: 0.54 LeeSin: 0.40 Malzahar: 0.40 Sett: 0.31\n===== COMP 10 ===== Zac: 0.64 Darius: 0.64 Malzahar: 0.64 Kobuko: 0.58 Poppy: 0.50 Jayce: 0.46 Aatrox: 0.37 Rammus: 0.32 Seraphine: 0.28 KSante: 0.21\n===== COMP 11 ===== Aatrox: 0.75 Ryze: 0.71 Udyr: 0.65 JarvanIV: 0.62 Akali: 0.56 Darius: 0.55 Senna: 0.49 Kennen: 0.49 Kobuko: 0.44 KSante: 0.41\nAfter looking at these comps, I then found the average place of each cluster.\ncluster 10 4.206250 4 4.497653 8 4.524462 11 4.526882 1 4.689005 2 4.739927 9 4.754364 3 4.783929 5 4.947175 6 5.016611 7 5.169611\n\nConclusions:\nThe most popular competitve comp was Comp 3, a Shen–Leona Vanguard team.\n===== COMP 3 ===== Shen: 0.89 Leona: 0.85 Rell: 0.84 Swain: 0.83 XinZhao: 0.81 Braum: 0.66 Garen: 0.43 Seraphine: 0.38 TwistedFate: 0.37 Zyra: 0.31\nShen, Leona, Rell, Swain, and Xin Zhao form a stable and highly repeatable core, each appearing in over 80% of compositions in this cluster. Braum serves as a common secondary unit, while Garen, Seraphine, Twisted Fate, and Zyra act as niche and/or situational inclusions.\nBecause this cluster is the largest, it represents the most frequently played comp among top-ranked players. However, despite its popularity, Comp 3 had only the fourth-best average placement, indicating that frequency does not necessarily equate to performance.\nThe highest placing was comp 10: ===== COMP 10 ===== Zac: 0.64 Darius: 0.64 Malzahar: 0.64 Kobuko: 0.58 Poppy: 0.50 Jayce: 0.46 Aatrox: 0.37 Rammus: 0.32 Seraphine: 0.28 KSante: 0.21\nThis comp is a tank/bruiser-oriented lineup anchored by Zac, Darius, Malzahar, and Kobuko. Interestingly, Comp 10 shows comparatively low internal cohesion—its most frequent champions appear only 64% of the time. The lack of a clearly defined, consistent core likely contributes to its lower pick rate, even though it performs extremely well when executed.\nComparing the most popular and best performing comp leads to some results of the metagame. There is almost no overlap between the top-performing and the most popular comps, with Seraphine being the only shared champion—and only as a low-frequency flex unit in both. Also, the placement range across all clusters is narrow (less than one position), indicating a highly balanced metagame at the top level of competitive play.\n\n\n\nCode\n\n#Import packages this file uses\nimport pandas as pd\nimport ast\nimport hdbscan\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n\ndfTFT = pd.read_csv(\"../data/processed-data/TFT_processed-data.csv\")\ndfTFT.head(5)\n\n\n\n\n\n\n\n\nmatch_id\npuuid\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngame_version\ngold_left\ntraits\n...\ntotal_items\nRelease_Version\ntotal_traits\ntotal_units\ntop1\ntop4\nbottom4\nchampions\nraw_champions\nchampion_list\n\n\n\n\n0\nNA1_5412752266\nzkxtkj27xwoevll2byguugjlpldforxw6vscn82z0s4m4w...\n8\n9\n1591.827148\n40\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n1\n[{'name': 'TFT15_Bastion', 'num_units': 1, 'st...\n...\n13\n15.22\n14\n9\n0\n0\n1\n[]\n['TFT15_Aatrox', 'TFT15_DrMundo', 'TFT15_Vi', ...\n['Aatrox', 'DrMundo', 'Vi', 'Udyr', 'Sett', 'B...\n\n\n1\nNA1_5412752266\n5brr3jrvsrxprqoi2u1vdib7uhyvqh9dbldf7a1dv9_uvw...\n1\n9\n2174.608154\n173\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n61\n[{'name': 'TFT15_DragonFist', 'num_units': 1, ...\n...\n17\n15.22\n10\n10\n1\n1\n0\n[]\n['TFT15_Kayle', 'TFT15_Zac', 'TFT15_Aatrox', '...\n['Kayle', 'Zac', 'Aatrox', 'Gangplank', 'Udyr'...\n\n\n2\nNA1_5412752266\nilqfyw7mnea2shfj5mvz4yhh6whmkrhoqa8m8otzxvtn71...\n6\n9\n1815.780273\n80\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n0\n[{'name': 'TFT15_Bastion', 'num_units': 4, 'st...\n...\n11\n15.22\n9\n10\n0\n0\n1\n[]\n['TFT15_Ezreal', 'TFT15_Garen', 'TFT15_Rell', ...\n['Ezreal', 'Garen', 'Rell', 'Rakan', 'Caitlyn'...\n\n\n3\nNA1_5412752266\nyja8q8aza0xq_s9hebroe4adddezonw0abvoc5gprzgmqw...\n3\n9\n2032.849243\n124\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n9\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n12\n15.22\n8\n9\n0\n1\n0\n[]\n['TFT15_Naafiri', 'TFT15_Lux', 'TFT15_XinZhao'...\n['Naafiri', 'Lux', 'XinZhao', 'Viego', 'Samira...\n\n\n4\nNA1_5412752266\noubcpr6kdwc2r4zz_msa6ftlulg14fqnrjj9pglbs2z4on...\n5\n8\n1818.687256\n84\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n41\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n13\n15.22\n7\n8\n0\n0\n1\n[]\n['TFT15_Ezreal', 'TFT15_Garen', 'TFT15_Syndra'...\n['Ezreal', 'Garen', 'Syndra', 'Rakan', 'Malzah...\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n#Only use Release Version 15.22\ndfTFT = dfTFT[dfTFT[\"Release_Version\"] == 15.22]\n\n\ndfTFT.shape\n\n(32455, 31)\n\n\n\n\nThis size is a massive issue. Way to much for me to run.\n\n#Convert Python Lists \ndfTFT[\"champion_list\"] = dfTFT[\"champion_list\"].apply(ast.literal_eval)\n\n\n#Drop duplicates of the champion comps. This should make the size manageable\ndfTFT = dfTFT.drop_duplicates(subset=[\"champion_list\"])\ndfTFT.shape\n\n(10757, 31)\n\n\n\n#Transform champion_list for HDBScan \nmlb = MultiLabelBinarizer()\nX = mlb.fit_transform(dfTFT[\"champion_list\"])\nX.shape\n\n#Reduce dimensionality for PCA. Tried it but after removing duplicates I dont think its necessary\npca = PCA(n_components = 25)\nX1 = pca.fit_transform(X)\n\n#See variance ratio\nsum(pca.explained_variance_ratio_)\n\n0.8263144237799275\n\n\n\nimport pandas as pd\nimport ast\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import pairwise_distances\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import fcluster\n\n#Get the Jaccard distances between champions\n#D = pairwise_distances(X1.astype(bool), metric=\"jaccard\")\n\nfrom sklearn.metrics import pairwise_distances\n\n#dist = euclidean not jaccard due to PCA.\nD_pca = pairwise_distances(X1, metric=\"euclidean\")\n\n#Then convert matrix to condensed form due to scipy requirements\nD_condensed = D_pca[np.triu_indices(len(D_pca), k=1)]\n\nZ = linkage(D_condensed, method=\"average\")\nlabels = fcluster(Z, t=15, criterion=\"maxclust\")\ndfTFT[\"cluster\"] = labels\n\n\nfor t in [2, 3, 4, 5]:\n    labels = fcluster(Z, t=t, criterion=\"distance\")\n    print(\"t =\", t, \"→ clusters =\", len(np.unique(labels)))\n\nt = 2 → clusters = 228\nt = 3 → clusters = 11\nt = 4 → clusters = 1\nt = 5 → clusters = 1\n\n\n\nlabels = fcluster(Z, t=3, criterion=\"distance\")\ndfTFT[\"cluster\"] = labels\ndfTFT[\"cluster\"].value_counts()\n\ncluster\n3     1680\n6     1505\n5     1363\n10     960\n1      955\n4      852\n2      819\n9      802\n11     744\n7      566\n8      511\nName: count, dtype: int64\n\n\n\nclusters = sorted(dfTFT[\"cluster\"].unique())\n\nfor cid in clusters:\n    subset = X[dfTFT[\"cluster\"] == cid]\n    freq = subset.mean(axis=0)\n\n    print(f\"\\n===== COMP {cid} =====\")\n    top_idx = freq.argsort()[::-1][:10]\n    for idx in top_idx:\n        print(f\"  {mlb.classes_[idx]}: {freq[idx]:.2f}\")\n\n\n===== COMP 1 =====\n  Poppy: 0.95\n  Jinx: 0.95\n  Neeko: 0.81\n  Rell: 0.75\n  Kobuko: 0.72\n  KSante: 0.68\n  Xayah: 0.52\n  Seraphine: 0.48\n  Varus: 0.47\n  Braum: 0.46\n\n===== COMP 2 =====\n  Garen: 0.95\n  Rakan: 0.90\n  Ezreal: 0.88\n  Leona: 0.84\n  Yuumi: 0.83\n  Jayce: 0.57\n  Katarina: 0.53\n  Ryze: 0.31\n  Malzahar: 0.29\n  KSante: 0.29\n\n===== COMP 3 =====\n  Shen: 0.89\n  Leona: 0.85\n  Rell: 0.84\n  Swain: 0.83\n  XinZhao: 0.81\n  Braum: 0.66\n  Garen: 0.43\n  Seraphine: 0.38\n  TwistedFate: 0.37\n  Zyra: 0.31\n\n===== COMP 4 =====\n  Samira: 0.93\n  Sett: 0.92\n  Volibear: 0.77\n  Naafiri: 0.73\n  XinZhao: 0.72\n  Lux: 0.54\n  Gwen: 0.48\n  Braum: 0.46\n  Viego: 0.40\n  Shen: 0.38\n\n===== COMP 5 =====\n  DrMundo: 0.93\n  Udyr: 0.91\n  Sett: 0.87\n  Naafiri: 0.80\n  Vi: 0.71\n  LeeSin: 0.71\n  Braum: 0.54\n  TwistedFate: 0.53\n  Zyra: 0.45\n  Aatrox: 0.44\n\n===== COMP 6 =====\n  Swain: 0.91\n  Janna: 0.90\n  Ashe: 0.85\n  Vi: 0.77\n  Zyra: 0.71\n  KSante: 0.69\n  JarvanIV: 0.67\n  Braum: 0.55\n  Udyr: 0.53\n  LeeSin: 0.47\n\n===== COMP 7 =====\n  Janna: 0.83\n  Malphite: 0.81\n  Sivir: 0.60\n  Ziggs: 0.57\n  Shen: 0.57\n  Ryze: 0.46\n  JarvanIV: 0.44\n  Neeko: 0.41\n  KSante: 0.34\n  Kennen: 0.31\n\n===== COMP 8 =====\n  Karma: 0.97\n  Lucian: 0.94\n  JarvanIV: 0.90\n  Ryze: 0.66\n  Swain: 0.51\n  Aatrox: 0.49\n  Senna: 0.46\n  Gwen: 0.44\n  Lux: 0.43\n  Gangplank: 0.41\n\n===== COMP 9 =====\n  Kayle: 0.82\n  Udyr: 0.79\n  Zac: 0.72\n  Jhin: 0.62\n  Aatrox: 0.60\n  Varus: 0.54\n  KSante: 0.54\n  LeeSin: 0.40\n  Malzahar: 0.40\n  Sett: 0.31\n\n===== COMP 10 =====\n  Zac: 0.64\n  Darius: 0.64\n  Malzahar: 0.64\n  Kobuko: 0.58\n  Poppy: 0.50\n  Jayce: 0.46\n  Aatrox: 0.37\n  Rammus: 0.32\n  Seraphine: 0.28\n  KSante: 0.21\n\n===== COMP 11 =====\n  Aatrox: 0.75\n  Ryze: 0.71\n  Udyr: 0.65\n  JarvanIV: 0.62\n  Akali: 0.56\n  Darius: 0.55\n  Senna: 0.49\n  Kennen: 0.49\n  Kobuko: 0.44\n  KSante: 0.41\n\n\n\nplace = dfTFT.groupby(\"cluster\")[\"placement\"].mean().sort_values()\nprint(place)\n\ncluster\n10    4.206250\n4     4.497653\n8     4.524462\n11    4.526882\n1     4.689005\n2     4.739927\n9     4.754364\n3     4.783929\n5     4.947175\n6     5.016611\n7     5.169611\nName: placement, dtype: float64\n\n\n\n#HDBSCAN\nclusterer = hdbscan.HDBSCAN(\n    min_cluster_size=150,\n    min_samples=20, \n    metric='jaccard', \n    cluster_selection_epsilon=0.0,\n    cluster_selection_method='leaf')\n\n#Note: We Are using X. Removal of duplicates made dataset manageable\nlabels = clusterer.fit_predict(X)\n\n#Look at Cluster sizs\nprint(pd.Series(labels).value_counts())\n\n\nchamp_names = mlb.classes_\n\ndef describe_clusters(X_bin, labels, champ_names, top_n = 10):\n    df_bin = pd.DataFrame(X_bin, columns=champ_names)\n    results = {}\n\n    for cluster_id in sorted(set(labels)):\n        if cluster_id == -1:\n            print(\"\\n=== Noise (cluster -1) ===\")\n        else:\n            print(f\"\\n=== Cluster {cluster_id} ===\")\n        \n        rows = df_bin[labels == cluster_id]\n        freq = rows.mean().sort_values(ascending=False)\n        \n        print(freq.head(top_n))\n        results[cluster_id] = freq.head(top_n)\n\n    return results\n\ncluster_summary = describe_clusters(X, labels, champ_names)\n\n\n#Show clusters via TSNE\ntsne = TSNE(n_components = 2, perplexity = 30, random_state = 42)\nemb = tsne.fit_transform(X)\n\nplt.figure(figsize=(10,7))\nplt.scatter(emb[:,0], emb[:,1], c = labels, s = 3, cmap = 'tab20')\nplt.title(\"t-SNE of TFT Boards (colored by PCA+Hierarchical clusters)\")\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "#Introduction\nThis analyses performs basic EDA to provide a high level picture of the cleaned TFT dataset. First, I am only looking at Version 15.22. Different game versions have balance patches which can affect gameplay in major ways.\nThis EDA begins with a basic info and descriptive statistics of the int and float fields. Then a histogram was made to get a general view of the distributions.The results of this showed some high degree of bias so the skew and kurtosis was taken. num_3star, num_cost3, 2.258098, 3.439884 had skew &gt; 1. For kurtosis, game_length = 29 and gold_left = 12. Based on these results, gold_left was an incredibly biased varaible i needed to consider when performing supervised and unsupervised learning. I also made a correlation matrix, and pairplot to see if any notable relationships among data was formed.\nFinally, to get a sense of unit popularity and team comps, I created a bar chart of the top 10 most used champions and a correlation heatmap of units. The most popular units were Braum, KSante, Udyr, Swain, Zyra, JarvanIV, Leona, LeeSin, Vi, and Janna.\nAs for the heatmap results, there were some notable correlations in Braum + Swain, Braum + Zyra, Janna + JarvanIV. When I try to determine comps, I wouldlike to see if these champions are frequently seen or the pairings I found are present.\n\nCode\n\nIntroduction and Motivation: The purpose of this notebook is to perform Exploratory Data Analysis (EDA) on the processed TFT gameplay dataset. The various analytics and tests done are done as tests for potential modeling ideas.\n\nOverview of Methods: Ttest, Bar Graphs, Heatmaps\n\n\n#import packages this file uses\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport itertools\nfrom collections import Counter\nfrom scipy import stats\n\n\n#Import singlar combined csv file from raw data step as a pandas df\ndfTFT = pd.read_csv(\"../data/processed-data/TFT_processed-data.csv\")\n\n\n#Only Use Version 15.22. Comparing versions isnt a one to one comparison\ndfTFT = dfTFT[dfTFT[\"Release_Version\"] == 15.22]\n\n\n#Note: top1, top4, bottom4 are all binarys. \ndfTFT.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32455 entries, 0 to 48667\nData columns (total 31 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   match_id         32455 non-null  object \n 1   puuid            32455 non-null  object \n 2   placement        32455 non-null  int64  \n 3   level            32455 non-null  int64  \n 4   time_eliminated  32455 non-null  float64\n 5   total_damage     32455 non-null  int64  \n 6   game_length      32455 non-null  float64\n 7   game_version     32455 non-null  object \n 8   gold_left        32455 non-null  int64  \n 9   traits           32455 non-null  object \n 10  units            32455 non-null  object \n 11  num_traits       32455 non-null  int64  \n 12  num_units        32455 non-null  int64  \n 13  num_1star        32455 non-null  int64  \n 14  num_2star        32455 non-null  int64  \n 15  num_3star        32455 non-null  int64  \n 16  num_cost1        32455 non-null  int64  \n 17  num_cost2        32455 non-null  int64  \n 18  num_cost3        32455 non-null  int64  \n 19  num_cost4        32455 non-null  int64  \n 20  num_cost5        32455 non-null  int64  \n 21  total_items      32455 non-null  int64  \n 22  Release_Version  32455 non-null  float64\n 23  total_traits     32455 non-null  int64  \n 24  total_units      32455 non-null  int64  \n 25  top1             32455 non-null  int64  \n 26  top4             32455 non-null  int64  \n 27  bottom4          32455 non-null  int64  \n 28  champions        32455 non-null  object \n 29  raw_champions    32455 non-null  object \n 30  champion_list    32455 non-null  object \ndtypes: float64(3), int64(20), object(8)\nmemory usage: 7.9+ MB\n\n\n\n#Get decriptive stats of int and float fields\ndfTFT.describe(include=['int64','float64']).T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nplacement\n32455.0\n4.497088\n2.292538e+00\n1.000000\n2.000000\n4.000000\n6.000000\n8.000000\n\n\nlevel\n32455.0\n8.539670\n8.527264e-01\n1.000000\n8.000000\n9.000000\n9.000000\n10.000000\n\n\ntime_eliminated\n32455.0\n1886.681196\n2.750301e+02\n7.777373\n1682.768311\n1904.160767\n2090.404053\n2748.024658\n\n\ntotal_damage\n32455.0\n95.586597\n4.821405e+01\n0.000000\n59.000000\n89.000000\n128.000000\n270.000000\n\n\ngame_length\n32455.0\n2190.228240\n1.451466e+02\n7.777373\n2098.891357\n2187.416260\n2284.013916\n2756.449219\n\n\ngold_left\n32455.0\n8.620644\n1.544957e+01\n0.000000\n0.000000\n1.000000\n9.000000\n183.000000\n\n\nnum_traits\n32455.0\n10.354830\n2.287158e+00\n0.000000\n9.000000\n10.000000\n12.000000\n20.000000\n\n\nnum_units\n32455.0\n8.695671\n1.048188e+00\n0.000000\n8.000000\n9.000000\n9.000000\n13.000000\n\n\nnum_1star\n32455.0\n2.155754\n1.543375e+00\n0.000000\n1.000000\n2.000000\n3.000000\n10.000000\n\n\nnum_2star\n32455.0\n5.646927\n2.119941e+00\n0.000000\n4.000000\n6.000000\n7.000000\n12.000000\n\n\nnum_3star\n32455.0\n0.886489\n1.238142e+00\n0.000000\n0.000000\n0.000000\n2.000000\n7.000000\n\n\nnum_cost1\n32455.0\n1.506979\n8.895040e-01\n0.000000\n1.000000\n2.000000\n2.000000\n5.000000\n\n\nnum_cost2\n32455.0\n1.667971\n7.391302e-01\n0.000000\n1.000000\n2.000000\n2.000000\n6.000000\n\n\nnum_cost3\n32455.0\n1.549037\n1.033483e+00\n0.000000\n1.000000\n1.000000\n2.000000\n8.000000\n\n\nnum_cost4\n32455.0\n0.000000\n0.000000e+00\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nnum_cost5\n32455.0\n2.250932\n1.073448e+00\n0.000000\n1.000000\n2.000000\n3.000000\n6.000000\n\n\ntotal_items\n32455.0\n12.385210\n3.510498e+00\n0.000000\n10.000000\n12.000000\n14.000000\n30.000000\n\n\nRelease_Version\n32455.0\n15.220000\n1.254305e-11\n15.220000\n15.220000\n15.220000\n15.220000\n15.220000\n\n\ntotal_traits\n32455.0\n10.354830\n2.287158e+00\n0.000000\n9.000000\n10.000000\n12.000000\n20.000000\n\n\ntotal_units\n32455.0\n8.695671\n1.048188e+00\n0.000000\n8.000000\n9.000000\n9.000000\n13.000000\n\n\ntop1\n32455.0\n0.125682\n3.314954e-01\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\ntop4\n32455.0\n0.500447\n5.000075e-01\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nbottom4\n32455.0\n0.499553\n5.000075e-01\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n#Create Histograms of all int and float fields\nnum_cols = dfTFT.select_dtypes(include=['int64','float64']).columns\ndfTFT[num_cols].hist(bins = 20, figsize = (12,12))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nData Distribution\n\n#Histogram visually shows skew among a couple of fields \n#Lets determine the skew of num cols\ndfTFT[num_cols].skew().sort_values()\n\ngame_length       -2.035491\ntotal_units       -1.231842\nnum_units         -1.231842\nlevel             -0.769241\ntime_eliminated   -0.462091\ntotal_traits      -0.124075\nnum_traits        -0.124075\nnum_cost2         -0.092288\nnum_2star         -0.079416\nnum_cost5         -0.024680\ntop4              -0.001787\nRelease_Version    0.000000\nnum_cost4          0.000000\nplacement          0.000957\nbottom4            0.001787\nnum_cost1          0.216555\ntotal_damage       0.439562\ntotal_items        0.513083\nnum_1star          0.849210\nnum_3star          1.298909\nnum_cost3          1.776404\ntop1               2.258501\ngold_left          2.865710\ndtype: float64\n\n\n\n#Look at Kurtosis\ndfTFT[num_cols].kurt().sort_values()\n\ntop4               -2.000120\nbottom4            -2.000120\nplacement          -1.238937\nnum_cost5          -0.615949\nnum_2star          -0.486263\ntotal_damage       -0.400370\nRelease_Version     0.000000\nnum_cost4           0.000000\nnum_cost1           0.100252\nnum_traits          0.224385\ntotal_traits        0.224385\nnum_cost2           0.435328\nnum_1star           0.770351\nnum_3star           0.939878\ntime_eliminated     1.134067\ntotal_items         1.376238\nlevel               2.684690\ntop1                3.101018\nnum_cost3           4.749030\nnum_units           7.040564\ntotal_units         7.040564\ngold_left          12.030834\ngame_length        29.433887\ndtype: float64\n\n\n\nplt.figure(figsize=(20,20))\nsns.heatmap(dfTFT[num_cols].corr(),annot = True, cmap = 'coolwarm')\nplt.title(\"Correlation Matrix of Numerical Fields\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#Look at pairwise relationships\nsns.set(style = \"whitegrid\")\nsns.pairplot(dfTFT[['placement', 'level', 'total_damage', 'game_length']])\nplt.show()\n\n\n\n\n\n\n\n\nH0: Gold lacks a significant effect on getting first place HA: Gold has a significant effect on getting first place\n\n#Create ttest assesing the siginfignace of gold left on first place\n#Despite heavy skew, i wont normalize. \n\nwin = dfTFT[dfTFT[\"top1\"] == 1][\"gold_left\"]\nlose = dfTFT[dfTFT[\"top1\"] != 1][\"gold_left\"]\n\nstats.ttest_ind(win, lose, equal_var = False)\n\nTtestResult(statistic=13.296788071192763, pvalue=1.0967396807080814e-39, df=5107.305276300659)\n\n\nP val &lt; .05, so we reject the null hypothesis and can conclude that gold_left is significant for placement. This informs of modeling.\n\n#TTest indicates high statistical siginifgance. \n#Lets use a violin plot top and bottom cut players without normalzaling the highly skewed variable\n\ndfTFT[\"finish_group\"] = dfTFT[\"placement\"].apply(lambda x: \"Top 4\" if x &lt;= 4 else \"Bottom 4\")\ndfTFT[\"finish_group\"].value_counts()\n\nplt.figure(figsize = (10,10))\nsns.violinplot(data = dfTFT, x = \"finish_group\", y = \"gold_left\" , palette = \"Set2\")\n\nplt.title(\"Difference in gold economy between top 4 and bottom 4\", fontsize = 12)\nplt.xlabel(\"Placement Group\", fontsize = 12)\nplt.ylabel(\"Gold Left\", fontsize = 12)\n\nplt.tight_layout\nplt.show()\n\n/var/folders/6s/cxp6j6zd22s3tgzn53pm65900000gn/T/ipykernel_27037/3703048918.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(data = dfTFT, x = \"finish_group\", y = \"gold_left\" , palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n#Only use 15.22. This build of game only matters for these 2 visuals\ndfTFT[\"champion_list\"] = dfTFT[\"champion_list\"].apply(ast.literal_eval)\ncounter = Counter()\nfor lst in dfTFT['champion_list']:\n    counter.update(lst)\ntop = counter.most_common(10)\nnames = [x[0] for x in top]\ncounts = [x[1] for x in top]\n\nplt.figure(figsize=(10,10))\nplt.bar(names, counts)\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top 10 Most Used Champions\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nchampions = sorted({c for row in dfTFT[\"champion_list\"] for c in row})\nmatrix = pd.DataFrame(0, index = champions, columns = champions)\nfor champs in dfTFT[\"champion_list\"]:\n    for c1, c2 in itertools.combinations(champs, 2):\n        matrix.loc[c1, c2] += 1\n        matrix.loc[c2, c1] += 1\n\nfor c in champions:\n    matrix.loc[c, c] = dfTFT[\"champion_list\"].apply(lambda x: c in x).sum()\n\nprint(\"Champion Co-occurrence Matrix:\")\nprint(matrix)\n\nChampion Co-occurrence Matrix:\n        Aatrox  Ahri  Akali  Ashe  Braum  Caitlyn  Darius  DrMundo  Ezreal  \\\nAatrox    7023    23   1406   273    969       11    2972     1574      37   \nAhri        23   729      3    23    157        0      17        8       7   \nAkali     1406     3   1975    12     38        7    1711       27     113   \nAshe       273    23     12  4894   3030        1      29      407       3   \nBraum      969   157     38  3030  12614       42      63     3664     203   \n...        ...   ...    ...   ...    ...      ...     ...      ...     ...   \nYone       734     7     28    43    593        0      22      243       1   \nYuumi      107     6    113     5    297      499     229        2    3862   \nZac       2444     6    350    18    140       10    2861      261      61   \nZiggs      265     2     20    66    403        8      16       10       8   \nZyra      1108    46     98  3716   6918       12     260     2946     186   \n\n        Gangplank  ...  Viego  Volibear  Xayah  XinZhao  Yasuo  Yone  Yuumi  \\\nAatrox        894  ...    150       695     25       46    378   734    107   \nAhri            7  ...      1        19    217       47     17     7      6   \nAkali          29  ...     11        18      4       10     78    28    113   \nAshe          228  ...    230       113      2       48      9    43      5   \nBraum          53  ...    527      2881   1184     3700    679   593    297   \n...           ...  ...    ...       ...    ...      ...    ...   ...    ...   \nYone          616  ...     65       779    158      486    324  1546      2   \nYuumi           1  ...      0        25      7      107      9     2   4291   \nZac            23  ...     13        19     10        6      4    14    198   \nZiggs          13  ...      3        35      1       35     23    12      8   \nZyra           81  ...    105       712    126     1519    129   203    231   \n\n         Zac  Ziggs  Zyra  \nAatrox  2444    265  1108  \nAhri       6      2    46  \nAkali    350     20    98  \nAshe      18     66  3716  \nBraum    140    403  6918  \n...      ...    ...   ...  \nYone      14     12   203  \nYuumi    198      8   231  \nZac     3613      1   339  \nZiggs      1   1549   144  \nZyra     339    144  9162  \n\n[63 rows x 63 columns]\n\n\n\nplt.figure(figsize = (18, 14))\nsns.heatmap(matrix, cmap = \"Reds\", square = True, cbar = True)\nplt.title(\"Champion Co-occurrence Heatmap\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Introduction The purpose of this data cleaning page was to take the JSON files created in the data-collecting step, combine and flatten the data into a singular Pandas data frame. Data cleaning was an interative process I needed to add and alter throughout the Project. Once the Pandas dataframe was build, i needed to remove parts of the dataset, and create additional calculated fields that would be used in the EDA, supervised, and unsupervised learning sections.This final data frame was then saved as a csv file for both documentation and later use in the EDA step.\nProcess The raw JSON file consists of multi-level nested dictionaries and lists. To flatten these into a dataframe, I used Python.\nOnce the JSON files had been combined, the shape was (50825, 23). 8662 unique player IDs across 6388 game were captured which is still robust despite my limitations.\nI first looked at the data types. Data types were object, int64, and float 64. Time fields game_length and time_eliminated being float was acceptable as time was measured in seconds.\nMissing data was identified using this code line: dfRaw.notnull().mean() * 100 Which provided how many of a fields data was null.\nMy result was 100% except for the field augments which was 0. As augments had no data, it was removed. This was likely due an error on the data-collection process. I was unable to resolve this due to the Riot API having no data to pull when I reran due to the release of Build 16. The rest of my fields had no null values.\nOutlier Detection and Treatment and Normalization and Scaling were not performed at this time. I wanted my final cleaned dataset to not alter the data that exists. Instead, looking at outliers, normalization, scaling, skew and kurtosis are evaluated in the EDA section, and then performed during the modeling section.\nThe final dataset was saved as csv file for usage in EDA, unsupervised and supervised learning sections.\n\nCode\nProvide the source code used for this section of the project here.\nIf you’re using a package for code organization, you can import it at this point. However, make sure that the actual workflow steps—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\nIf relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\nRemember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on.\n\n#Import packages used in this notebook\nimport pandas as pd\nimport re\nimport numpy as np\nimport ast\nfrom pathlib import Path\nimport os\nimport json\n\n\n\nBegin by extracting all useful data from the JSON files\n\n\nThen combine them into one pandas data frame.\n\njson_dir = 'data/raw-data'\nrows = []\n\n#Create a lot of counting if statements\n\n#get count of tier\ndef count_stars(units, level):\n    return sum(1 for u in units if u.get(\"tier\") == level)\n\n#get total unit cost\ndef count_cost(units, cost):\n    return sum(1 for u in units if u.get(\"rarity\") == cost - 1)\n\n#get items\ndef count_items(units):\n    return sum(len(u.get(\"itemNames\", [])) for u in units)\n\n#get unit costs. Cost = rarity - 1\ndef unit_cost(units):\n    return [u.get(\"rarity\", -1) + 1 for u in units]\n\nfor filename in os.listdir(json_dir):\n    if filename.endswith(\".json\"):\n        filepath = os.path.join(json_dir, filename)\n        try:\n            with open(filepath, \"r\") as f:\n                data = json.load(f)\n        except Exception as e:\n            print(f\"Error reading {filename}: {e}\")\n            continue\n\n        match_id = data[\"metadata\"][\"match_id\"]\n        info = data[\"info\"]\n        game_length = info.get(\"game_length\")\n        game_version = info.get(\"game_version\")\n\n        for p in info[\"participants\"]:\n            #Start extracting the rows of csv file\n            units = p.get(\"units\", [])\n            traits = p.get(\"traits\", [])\n\n            #Basic Gameplay Info\n            rows.append({\n                \"match_id\": match_id,\n                \"puuid\": p[\"puuid\"],\n                \"placement\": p.get(\"placement\"),\n                \"level\": p.get(\"level\"),\n                \"time_eliminated\": p.get(\"time_eliminated\"),\n                \"total_damage\": p.get(\"total_damage_to_players\"),\n                \"game_length\": game_length,\n                \"game_version\": game_version,\n                \"gold_left\": p.get(\"gold_left\"),\n\n                #Unit, Trait\n                \"traits\": traits,\n                \"units\": units,\n                \"tot_traits\": len(traits),\n                \"tot_units\": len(units),\n\n                #Count of the units by star level\n                \"tot_1star\": count_stars(units, 1),\n                \"tot_champs\": count_stars(units, 2),\n                \"tot_champs\": count_stars(units, 3),\n\n                #Get count of units by cost\n                \"cost1\": count_cost(units, 1),\n                \"tot_cost2\": count_cost(units, 2),\n                \"tot_cost3\": count_cost(units, 3),\n                \"tot_cost4\": count_cost(units, 4),\n                \"tot_cost5\": count_cost(units, 5),\n\n                #How many Items\n                \"tot_items\": count_items(units),\n\n                #Get the Augments\n                \"augments\": p.get(\"augments\"),})\n\n#Create final DataFrame and faltten\ndf = pd.DataFrame(rows)\n\n\n#how many rows does the final df have?\nprint(\"Merged rows:\", len(df))\n\n#display the df\nprint(df.head())\n\n\n#Save df as csv \ndf.to_csv(\"data/raw/RIOT_rawdata.csv\", index=False)\n\n\n#Read combined csv file from folder raw as a pandas df\ndfRaw = pd.read_csv(\"../data-collection/data/raw/RIOT_rawdata.csv\")\n\n#Inital check of df\ndfRaw.head(5)\n\n\n\n\n\n\n\n\nmatch_id\npuuid\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngame_version\ngold_left\ntraits\n...\nnum_1star\nnum_2star\nnum_3star\nnum_cost1\nnum_cost2\nnum_cost3\nnum_cost4\nnum_cost5\ntotal_items\naugments\n\n\n\n\n0\nNA1_5412752266\nzkXtkj27xwOEvLL2bygUUGjlPlDFOrxW6vscN82z0s4m4w...\n8\n9\n1591.827148\n40\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n1\n[{'name': 'TFT15_Bastion', 'num_units': 1, 'st...\n...\n4\n5\n0\n1\n2\n1\n0\n1\n13\nNaN\n\n\n1\nNA1_5412752266\n5bRR3JrvSRXpRqOi2u1VDib7uHYVQH9DBlDF7A1dv9_uvw...\n1\n9\n2174.608154\n173\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n61\n[{'name': 'TFT15_DragonFist', 'num_units': 1, ...\n...\n4\n5\n1\n3\n1\n2\n0\n2\n17\nNaN\n\n\n2\nNA1_5412752266\nILqfYW7Mnea2sHFJ5mvZ4yhH6wHmkRhOqA8m8oTZXvtN71...\n6\n9\n1815.780273\n80\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n0\n[{'name': 'TFT15_Bastion', 'num_units': 4, 'st...\n...\n2\n7\n1\n3\n1\n3\n0\n2\n11\nNaN\n\n\n3\nNA1_5412752266\nyja8Q8Aza0XQ_s9heBROE4aDddezONw0abVOC5GpRZGMqw...\n3\n9\n2032.849243\n124\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n9\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n0\n9\n0\n1\n2\n1\n0\n3\n12\nNaN\n\n\n4\nNA1_5412752266\nOUBCpR6kdwc2R4zz_MSa6FTluLG14FQnrjJ9Pglbs2Z4ON...\n5\n8\n1818.687256\n84\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n41\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n1\n7\n0\n3\n1\n1\n0\n3\n13\nNaN\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n#See intitial shape\ndfRaw.shape\n\n(50825, 23)\n\n\n\n#How many games did the API acquire?\ndfRaw[\"match_id\"].nunique()\n\n6388\n\n\n\n#How many players did the API acquire?\ndfRaw[\"puuid\"].nunique()\n\n8662\n\n\n\n#Begin by dropping duplicates\ndfRaw = dfRaw.drop_duplicates()\ndfRaw.shape\n\n(50825, 23)\n\n\n\n#Check datatypes\ndfRaw.dtypes\n\nmatch_id            object\npuuid               object\nplacement            int64\nlevel                int64\ntime_eliminated    float64\ntotal_damage         int64\ngame_length        float64\ngame_version        object\ngold_left            int64\ntraits              object\nunits               object\nnum_traits           int64\nnum_units            int64\nnum_1star            int64\nnum_2star            int64\nnum_3star            int64\nnum_cost1            int64\nnum_cost2            int64\nnum_cost3            int64\nnum_cost4            int64\nnum_cost5            int64\ntotal_items          int64\naugments           float64\ndtype: object\n\n\n\n#get decriptive stats of int and float fields\ndfRaw.describe(include=['int64','float64'])\n\n\n\n\n\n\n\n\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngold_left\nnum_traits\nnum_units\nnum_1star\nnum_2star\nnum_3star\nnum_cost1\nnum_cost2\nnum_cost3\nnum_cost4\nnum_cost5\ntotal_items\naugments\n\n\n\n\ncount\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n50825.000000\n0.0\n\n\nmean\n4.497196\n8.549788\n1880.512909\n95.141564\n2186.548711\n9.397226\n10.366552\n8.678013\n2.132435\n5.626857\n0.912464\n1.515258\n1.654934\n1.547231\n0.004230\n2.192878\n12.576606\nNaN\n\n\nstd\n2.292515\n0.873291\n280.256322\n48.930045\n146.905622\n17.203921\n2.297279\n1.140197\n1.545226\n2.153924\n1.256414\n0.894001\n0.743277\n1.002328\n0.067867\n1.098274\n3.810764\nNaN\n\n\nmin\n1.000000\n1.000000\n7.777373\n0.000000\n7.777373\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\nNaN\n\n\n25%\n2.000000\n8.000000\n1676.638184\n58.000000\n2091.750244\n0.000000\n9.000000\n8.000000\n1.000000\n4.000000\n0.000000\n1.000000\n1.000000\n1.000000\n0.000000\n1.000000\n10.000000\nNaN\n\n\n50%\n4.000000\n9.000000\n1898.408813\n89.000000\n2184.151855\n2.000000\n10.000000\n9.000000\n2.000000\n6.000000\n0.000000\n2.000000\n2.000000\n1.000000\n0.000000\n2.000000\n12.000000\nNaN\n\n\n75%\n6.000000\n9.000000\n2086.252686\n128.000000\n2283.740723\n10.000000\n12.000000\n9.000000\n3.000000\n7.000000\n2.000000\n2.000000\n2.000000\n2.000000\n0.000000\n3.000000\n15.000000\nNaN\n\n\nmax\n8.000000\n10.000000\n3036.849609\n283.000000\n3048.027588\n438.000000\n20.000000\n15.000000\n11.000000\n14.000000\n7.000000\n7.000000\n6.000000\n8.000000\n2.000000\n9.000000\n36.000000\nNaN\n\n\n\n\n\n\n\n\n#Is all data from same version of game?\ndfRaw['game_version'].unique()\n\narray(['Linux Version 15.22.724.5161 (Nov 05 2025/16:11:29) [PUBLIC] &lt;Releases/15.22&gt;',\n       'Linux Version 15.21.721.8442 (Oct 24 2025/18:44:48) [PUBLIC] &lt;Releases/15.21&gt;',\n       'Linux Version 15.23.726.9074 (Nov 17 2025/11:33:30) [PUBLIC] &lt;Releases/15.23&gt;',\n       'Linux Version 15.22.723.8534 (Nov 03 2025/13:45:54) [PUBLIC] &lt;Releases/15.22&gt;',\n       'Linux Version 15.23.728.3286 (Nov 21 2025/16:26:55) [PUBLIC] &lt;Releases/15.23&gt;',\n       'Linux Version 15.22.723.1955 (Oct 30 2025/15:20:31) [PUBLIC] &lt;Releases/15.22&gt;',\n       'Linux Version 15.21.721.4012 (Oct 23 2025/12:15:03) [PUBLIC] &lt;Releases/15.21&gt;',\n       'Linux Version 15.20.719.0545 (Oct 14 2025/12:30:46) [PUBLIC] &lt;Releases/15.20&gt;',\n       'Linux Version 15.21.720.0925 (Oct 17 2025/16:00:09) [PUBLIC] &lt;Releases/15.21&gt;',\n       'Linux Version 15.21.721.0583 (Oct 22 2025/11:42:56) [PUBLIC] &lt;Releases/15.21&gt;'],\n      dtype=object)\n\n\n\n#As seen above, the Releases are slightly different. This is important because each release has a diff balance to the game\n#source: https://www.leagueoflegends.com/en-us/news/tags/teamfight-tactics-patch-notes/\n\n#lets pull the releases out of game_version and count the number of rows by version\ndfRaw[\"Release_Version\"] = dfRaw[\"game_version\"].str.extract(r\"&lt;Releases/([\\d\\.]+)&gt;\")\ndfRaw[\"Release_Version\"].value_counts()\n\nRelease_Version\n15.22    33808\n15.23     8986\n15.21     7972\n15.20       59\nName: count, dtype: int64\n\n\n\n#Remove 15.20 should be removed because it only has 59 records\n#15.22 is most robust when eval a single release\n#15.23 and 15.21 are kept incase I want to do a version comparison\n\ndfRaw = dfRaw[dfRaw[\"Release_Version\"] != \"15.20\"]\ndfRaw[\"Release_Version\"].value_counts()\n\nRelease_Version\n15.22    33808\n15.23     8986\n15.21     7972\nName: count, dtype: int64\n\n\n\n#check percentage of nulls in each row\ndfRaw.notnull().mean() * 100\n\nmatch_id           100.0\npuuid              100.0\nplacement          100.0\nlevel              100.0\ntime_eliminated    100.0\ntotal_damage       100.0\ngame_length        100.0\ngame_version       100.0\ngold_left          100.0\ntraits             100.0\nunits              100.0\nnum_traits         100.0\nnum_units          100.0\nnum_1star          100.0\nnum_2star          100.0\nnum_3star          100.0\nnum_cost1          100.0\nnum_cost2          100.0\nnum_cost3          100.0\nnum_cost4          100.0\nnum_cost5          100.0\ntotal_items        100.0\naugments             0.0\nRelease_Version    100.0\ndtype: float64\n\n\n\n#Auguments is only case of null data. But Auguments has 0 data popualted so get rid of it.\ndfRaw = dfRaw.drop([\"augments\"],axis = 1)\n\n\n\nThis might be error on my API script. But I dont have time to pull new data or fix this.\n\n#How much of the dataset is created by bots?\n#Bots are identified by having bot in puuid\n\ndfRaw['puuid'] = dfRaw['puuid'].astype(str).str.lower()\nbot_puuid = dfRaw['puuid'] == \"bot\"\nnum_bots = bot_puuid.sum()\nprint(\"Total bots:\", num_bots)\n\nTotal bots: 11\n\n\n\n#Remove bot data\ndfRaw = dfRaw[dfRaw['puuid'] != \"bot\"]\n\n\n#Begin creating useful fields for modeling\n\n\n#Traits and Units need to be converted into Python Objects\ndfRaw[\"traits\"] = dfRaw[\"traits\"].apply(ast.literal_eval)\ndfRaw[\"units\"] = dfRaw[\"units\"].apply(ast.literal_eval)\n\ndfRaw.head(5)\n\n\n\n\n\n\n\n\nmatch_id\npuuid\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngame_version\ngold_left\ntraits\n...\nnum_1star\nnum_2star\nnum_3star\nnum_cost1\nnum_cost2\nnum_cost3\nnum_cost4\nnum_cost5\ntotal_items\nRelease_Version\n\n\n\n\n0\nNA1_5412752266\nzkxtkj27xwoevll2byguugjlpldforxw6vscn82z0s4m4w...\n8\n9\n1591.827148\n40\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n1\n[{'name': 'TFT15_Bastion', 'num_units': 1, 'st...\n...\n4\n5\n0\n1\n2\n1\n0\n1\n13\n15.22\n\n\n1\nNA1_5412752266\n5brr3jrvsrxprqoi2u1vdib7uhyvqh9dbldf7a1dv9_uvw...\n1\n9\n2174.608154\n173\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n61\n[{'name': 'TFT15_DragonFist', 'num_units': 1, ...\n...\n4\n5\n1\n3\n1\n2\n0\n2\n17\n15.22\n\n\n2\nNA1_5412752266\nilqfyw7mnea2shfj5mvz4yhh6whmkrhoqa8m8otzxvtn71...\n6\n9\n1815.780273\n80\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n0\n[{'name': 'TFT15_Bastion', 'num_units': 4, 'st...\n...\n2\n7\n1\n3\n1\n3\n0\n2\n11\n15.22\n\n\n3\nNA1_5412752266\nyja8q8aza0xq_s9hebroe4adddezonw0abvoc5gprzgmqw...\n3\n9\n2032.849243\n124\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n9\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n0\n9\n0\n1\n2\n1\n0\n3\n12\n15.22\n\n\n4\nNA1_5412752266\noubcpr6kdwc2r4zz_msa6ftlulg14fqnrjj9pglbs2z4on...\n5\n8\n1818.687256\n84\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n41\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n1\n7\n0\n3\n1\n1\n0\n3\n13\n15.22\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n#Find count of traits and units\ndfRaw[\"total_traits\"] = dfRaw[\"traits\"].apply(lambda t: len(t))\ndfRaw[\"total_units\"] = dfRaw[\"units\"].apply(lambda u: len(u))\n\n#check\ndfRaw.head(5)\n\n\n\n\n\n\n\n\nmatch_id\npuuid\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngame_version\ngold_left\ntraits\n...\nnum_3star\nnum_cost1\nnum_cost2\nnum_cost3\nnum_cost4\nnum_cost5\ntotal_items\nRelease_Version\ntotal_traits\ntotal_units\n\n\n\n\n0\nNA1_5412752266\nzkxtkj27xwoevll2byguugjlpldforxw6vscn82z0s4m4w...\n8\n9\n1591.827148\n40\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n1\n[{'name': 'TFT15_Bastion', 'num_units': 1, 'st...\n...\n0\n1\n2\n1\n0\n1\n13\n15.22\n14\n9\n\n\n1\nNA1_5412752266\n5brr3jrvsrxprqoi2u1vdib7uhyvqh9dbldf7a1dv9_uvw...\n1\n9\n2174.608154\n173\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n61\n[{'name': 'TFT15_DragonFist', 'num_units': 1, ...\n...\n1\n3\n1\n2\n0\n2\n17\n15.22\n10\n10\n\n\n2\nNA1_5412752266\nilqfyw7mnea2shfj5mvz4yhh6whmkrhoqa8m8otzxvtn71...\n6\n9\n1815.780273\n80\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n0\n[{'name': 'TFT15_Bastion', 'num_units': 4, 'st...\n...\n1\n3\n1\n3\n0\n2\n11\n15.22\n9\n10\n\n\n3\nNA1_5412752266\nyja8q8aza0xq_s9hebroe4adddezonw0abvoc5gprzgmqw...\n3\n9\n2032.849243\n124\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n9\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n0\n1\n2\n1\n0\n3\n12\n15.22\n8\n9\n\n\n4\nNA1_5412752266\noubcpr6kdwc2r4zz_msa6ftlulg14fqnrjj9pglbs2z4on...\n5\n8\n1818.687256\n84\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n41\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n0\n3\n1\n1\n0\n3\n13\n15.22\n7\n8\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n#create binary that shows if placed 1st\ndfRaw[\"top1\"] = (dfRaw[\"placement\"] == 1).astype(int)\n\n#create binary that shows if placed top 4\ndfRaw[\"top4\"] = (dfRaw[\"placement\"] &lt;= 4).astype(int)\n\n#create binary that shows if placed bottom 4 (5-8)\ndfRaw[\"bottom4\"] = (dfRaw[\"placement\"] &gt;= 5).astype(int)\n\n#check results\ndfRaw.head(5)\n\n\n\n\n\n\n\n\nmatch_id\npuuid\nplacement\nlevel\ntime_eliminated\ntotal_damage\ngame_length\ngame_version\ngold_left\ntraits\n...\nnum_cost3\nnum_cost4\nnum_cost5\ntotal_items\nRelease_Version\ntotal_traits\ntotal_units\ntop1\ntop4\nbottom4\n\n\n\n\n0\nNA1_5412752266\nzkxtkj27xwoevll2byguugjlpldforxw6vscn82z0s4m4w...\n8\n9\n1591.827148\n40\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n1\n[{'name': 'TFT15_Bastion', 'num_units': 1, 'st...\n...\n1\n0\n1\n13\n15.22\n14\n9\n0\n0\n1\n\n\n1\nNA1_5412752266\n5brr3jrvsrxprqoi2u1vdib7uhyvqh9dbldf7a1dv9_uvw...\n1\n9\n2174.608154\n173\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n61\n[{'name': 'TFT15_DragonFist', 'num_units': 1, ...\n...\n2\n0\n2\n17\n15.22\n10\n10\n1\n1\n0\n\n\n2\nNA1_5412752266\nilqfyw7mnea2shfj5mvz4yhh6whmkrhoqa8m8otzxvtn71...\n6\n9\n1815.780273\n80\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n0\n[{'name': 'TFT15_Bastion', 'num_units': 4, 'st...\n...\n3\n0\n2\n11\n15.22\n9\n10\n0\n0\n1\n\n\n3\nNA1_5412752266\nyja8q8aza0xq_s9hebroe4adddezonw0abvoc5gprzgmqw...\n3\n9\n2032.849243\n124\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n9\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n1\n0\n3\n12\n15.22\n8\n9\n0\n1\n0\n\n\n4\nNA1_5412752266\noubcpr6kdwc2r4zz_msa6ftlulg14fqnrjj9pglbs2z4on...\n5\n8\n1818.687256\n84\n2182.824219\nLinux Version 15.22.724.5161 (Nov 05 2025/16:1...\n41\n[{'name': 'TFT15_Bastion', 'num_units': 2, 'st...\n...\n1\n0\n3\n13\n15.22\n7\n8\n0\n0\n1\n\n\n\n\n5 rows × 28 columns\n\n\n\n\n\nWhen looking at data, champions have in front of name TFT7_ or TFT7b_.\n\n\nThis indicates that they are from Set 7.\nSet 7 and Set 15 are completly different versions. Our results will be invalid if both are used. So, remove Set 7 data.\n\n#When looking at data, champions have in front of name TFT7_ or TFT7b_. \n#This indicates that they are from Set 7.\n#Set 7 and Set 15 are completly different versions. So we need to get rid of all Set 7.\n\ndef get_champ(row):\n    if isinstance(row, list):\n        return [u[\"character_id\"] for u in row]\n    return []\n\n#Using field units and get_champions, make field raw_champions\ndfRaw[\"raw_champions\"] = dfRaw[\"units\"].apply(get_champions)\nset7_champs = (\n    dfRaw[\"raw_champions\"].explode().dropna().unique())\n\n#Identify Set 7\nset7_champs = [c for c in set7_champs if c.startswith((\"TFT7_\", \"TFT7b_\"))]\n\n#Remove Set 7.\ndfRaw = dfRaw[\n    ~dfRaw[\"raw_champions\"].apply(lambda lst: any(c in set7_champs for c in lst))\n].copy()\n\n\n#Now get rid of the TFT15_ in front of the champions names.\ndef clean_names(lst):\n    return [c.replace(\"TFT15_\", \"\").replace(\"tft15_\", \"\") for c in lst]\ndfRaw[\"champion_list\"] = dfRaw[\"raw_champions\"].apply(clean_names)\n\n#check Champions\ndfRaw[\"champion_list\"].explode().dropna().unique()\n\narray(['Aatrox', 'DrMundo', 'Vi', 'Udyr', 'Sett', 'Braum', 'leesin',\n       'TwistedFate', 'Zyra', 'Kayle', 'Zac', 'Gangplank', 'Viego',\n       'Ashe', 'JarvanIV', 'Galio', 'Ezreal', 'Garen', 'Rell', 'Rakan',\n       'Caitlyn', 'Jayce', 'Neeko', 'Leona', 'Yuumi', 'Naafiri', 'Lux',\n       'XinZhao', 'Samira', 'Volibear', 'Gwen', 'Syndra', 'Malzahar',\n       'KSante', 'Lucian', 'Senna', 'Ryze', 'Karma', 'Yone', 'Kennen',\n       'Ahri', 'Swain', 'smolder', 'Kalista', 'Kobuko', 'Shen', 'Jinx',\n       'Poppy', 'Varus', 'Seraphine', 'Gnar', 'Xayah', 'Janna',\n       'Katarina', 'Smolder', 'Sivir', 'Malphite', 'Ziggs', 'KogMaw',\n       'Yasuo', 'rammus', 'Akali', 'Darius', 'Jhin', 'kogmaw', 'KaiSa',\n       'Rammus', 'Ekko', 'lulu'], dtype=object)\n\n\n\n\nThis is better, but notice some are in lowercase like lulu, rammus, kogmaw\n\n\nNeed to address this, kogmaw and KogMaw might be treated as seperate entities when I perform other functions.\n\n#These are the set 15 champions\n#source: https://tftactics.gg/champions/\nset15 = ['Aatrox','Ahri','Akali','Ashe',\n                'Braum','Caitlyn','Darius','DrMundo',\n                'Ezreal','Gangplank','Garen','Gnar','Gwen',\n                'Janna','JarvanIV','Jayce','Jhin','Jinx',\n                'Kalista','Karma','Katarina','Kayle','Kennen',\n                'Kobuko','KogMaw','KSante','LeeSin','Leona','Lucian',\n                'Lulu','Lux','Malphite','Malzahar','Naafiri','Neeko',\n                'Poppy','Rakan','Rammus','Rell','Ryze','Samira','Senna',\n                'Seraphine','Sett','Shen','Sivir','Smolder','Swain',\n                'Syndra','TwistedFate','Udyr','Varus','Viego','Vi',\n                'Volibear','Xayah','XinZhao','Yasuo','Yone','Yuumi',\n                'Zac','Ziggs','Zyra']\n\n\nmapping = {c.lower(): c for c in set15}\n\n#normalize for cases\ndef norm(lst):\n    cleaned = []\n    for c in lst:\n        if isinstance(c, str):\n            key = c.lower()\n            if key in mapping:\n                cleaned.append(mapping[key])\n    return cleaned\n\ndfRaw[\"champion_list\"] = dfRaw[\"champion_list\"].apply(norm)\n\n\n#Check the champions.\nsorted(dfRaw[\"champion_list\"].explode().dropna().unique())\n\n['Aatrox',\n 'Ahri',\n 'Akali',\n 'Ashe',\n 'Braum',\n 'Caitlyn',\n 'Darius',\n 'DrMundo',\n 'Ezreal',\n 'Gangplank',\n 'Garen',\n 'Gnar',\n 'Gwen',\n 'Janna',\n 'JarvanIV',\n 'Jayce',\n 'Jhin',\n 'Jinx',\n 'KSante',\n 'Kalista',\n 'Karma',\n 'Katarina',\n 'Kayle',\n 'Kennen',\n 'Kobuko',\n 'KogMaw',\n 'LeeSin',\n 'Leona',\n 'Lucian',\n 'Lulu',\n 'Lux',\n 'Malphite',\n 'Malzahar',\n 'Naafiri',\n 'Neeko',\n 'Poppy',\n 'Rakan',\n 'Rammus',\n 'Rell',\n 'Ryze',\n 'Samira',\n 'Senna',\n 'Seraphine',\n 'Sett',\n 'Shen',\n 'Sivir',\n 'Smolder',\n 'Swain',\n 'Syndra',\n 'TwistedFate',\n 'Udyr',\n 'Varus',\n 'Vi',\n 'Viego',\n 'Volibear',\n 'Xayah',\n 'XinZhao',\n 'Yasuo',\n 'Yone',\n 'Yuumi',\n 'Zac',\n 'Ziggs',\n 'Zyra']\n\n\n\n#See final shape\ndfRaw.shape\n\n(48692, 30)\n\n\n\n#How many players are left?\ndfRaw[\"puuid\"].nunique()\n\n6994\n\n\n\n#How many matches are left?\ndfRaw[\"puuid\"].nunique()\n\n\n#Save final dataframe as a csv file to data/processed-data\n#This csv file will be used for EDA\n\n#Set where csv file is going\nout_dir = Path(\"../data/processed-data\")\nout_dir.mkdir(parents=True, exist_ok=True)\nout_path = out_dir / \"TFT_processed-data.csv\"\n\n#Export dataframe to CSV at location defined by out_dir\ndfRaw.to_csv(out_path, index=False)"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The goal of this project is to identify the most important factors that determine player placement in high-level Teamfight Tactics (TFT) games. Two related research questions were explored:\n\nWhich variables have the greatest impact on overall placement among top-ranked players?\nWhich variables are most predictive of finishing in first place?\n\nTo answer these questions, two machine learning models were used:\n\nRandom Forest Regressor — to predict a player’s numerical placement\nRandom Forest Classifier — to predict whether a player finished first (binary outcome)\n\nRandom Forest models were chosen because they handle nonlinear relationships, do not require normalization or scaling, and naturally provide feature importance rankings. This makes them well-suited for gameplay-derived variables that differ widely in scale and distribution.\nThe dataset included various gameplay metrics such as gold remaining, number of items used, total champions fielded, and distribution of champion star levels and costs. These features were used to train both models after performing standard train–test splits.\nWhat are the most significant factors that determine placement at top level play? What are the most significant factors that determine first place?\nTo best awnser this, I used Random Forest Regressor. My choices was due to the fact Random Forest Regressor does not require normalization, feature scaling, or linearity assumptions. For determining first place, I decided on Random Forest Classifer given that the outcome Im trying to predict is a binary categorical variable."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction",
    "href": "technical-details/supervised-learning/instructions.html#introduction",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The goal of this project is to identify the most important factors that determine player placement in high-level Teamfight Tactics (TFT) games. Two related research questions were explored:\n\nWhich variables have the greatest impact on overall placement among top-ranked players?\nWhich variables are most predictive of finishing in first place?\n\nTo answer these questions, two machine learning models were used:\n\nRandom Forest Regressor — to predict a player’s numerical placement\nRandom Forest Classifier — to predict whether a player finished first (binary outcome)\n\nRandom Forest models were chosen because they handle nonlinear relationships, do not require normalization or scaling, and naturally provide feature importance rankings. This makes them well-suited for gameplay-derived variables that differ widely in scale and distribution.\nThe dataset included various gameplay metrics such as gold remaining, number of items used, total champions fielded, and distribution of champion star levels and costs. These features were used to train both models after performing standard train–test splits.\nWhat are the most significant factors that determine placement at top level play? What are the most significant factors that determine first place?\nTo best awnser this, I used Random Forest Regressor. My choices was due to the fact Random Forest Regressor does not require normalization, feature scaling, or linearity assumptions. For determining first place, I decided on Random Forest Classifer given that the outcome Im trying to predict is a binary categorical variable."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#methods",
    "href": "technical-details/supervised-learning/instructions.html#methods",
    "title": "DSAN-5000: Project",
    "section": "Methods",
    "text": "Methods\nRandom Forest Regressor: - A Random Forest Regressor is an ensemble learning method that builds many decision trees and averages their predictions. It performs well on nonlinear data and identifies which features contribute most to prediction accuracy.\nRandom Forest Classifier: - A Random Forest Classifier uses the same ensemble strategy but outputs categorical predictions. In this case, the model predicted whether a player achieved first place (1) or not (0).\nFeature Definitions The following gameplay features were used:\ngold_left — Gold remaining when the game ends total_items — Number of completed or component items used num_units — Total number of champions deployed num_1star / 2star / 3star — Count of champions by star level num_cost1 / 2 / 3 / 4 / 5 — Count of champions by shop cost\nSeveral variables were intentionally omitted, including time_eliminated, total_damage, and game_length, because they strongly correlate with placement and would cause leakage, making the model trivially predict outcomes.\nModel Training\nThe dataset was split using train_test_split. For the classifier predicting first place, the split used: stratify = y1 Stratify was applied because first place finishes accounted for 12.5% of the total dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "DSAN-5000: Project",
    "section": "Results",
    "text": "Results\nRandom Forest Regressor Initial model performance: MAE = 0.548 MSE = 0.536 RMSE = 0.732\nAfter the train–test split:\nMAE = 1.168 MSE = 2.261 RMSE = 1.504\nDespite omitted variable bias, these values indicate reasonably strong performance given the inherent randomness of TFT.\nImportant Predictors:\ntotal_items ≈ 0.30 num_2star gold_left num_units num_3star\nThe most dominant factor was items.\nRandom Forest Classifier\nBelow are metrics of validity:\nAccuracy = 0.880 Precision = 0.531 Recall = 0.386 F1 = 0.447 ROC-AUC = 0.878\nAccuracy and ROC-AUC are strong, but Precision, Recall, and F1 are relatively weak, likely due to the rarity of first-place finishes and the difficulty of modeling endgame volatility.\nThe most important factors were: gold_left ≈ 0.25 total_items num_units num_2star"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#conclusions",
    "href": "technical-details/supervised-learning/instructions.html#conclusions",
    "title": "DSAN-5000: Project",
    "section": "Conclusions:",
    "text": "Conclusions:\nAcross both models, four features consistently emerged as the most important:\nItems Gold left over Number of 2-star champions Total units deployed\nInterestingly, the total number of champions was less predictive than itemization and gold economy. This suggests that TFT success at top level play depends more on quality of units (upgrades + items) rather than simply quantity of units."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools were used to brainstorm ideas and provide feedback and refine the project plan.\nTo find useful documentation that would help improve my understanding of TFT."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation\nUsed LLM’s to troubleshoot coding errors/bugs I didnt know how to resolve."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#api",
    "href": "technical-details/llm-usage-log.html#api",
    "title": "LLM usage log",
    "section": "API",
    "text": "API\n\nTroubleshoot Riot API key failing"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nObjectives: The objective of the data-collection processing is to gather the data for this project. The deseriable outcome is gathering json files from the Riot Games API containing players most recent gameplay data."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nAll of the data for this project was collected directly from the official Riot Games Teamfight Tactics (TFT) API, which is a public API maintained by Riot Games. It provides real-time and historical game data for TFT players, matches, and ranked ladders.\nData Tables Used:\nLeague endpoints: retrieves Master, Grandmaster, and Challenger ranked players Match endpoints: retrieves match history and full match JSON data PUUID lookup: maps players to their unique global identifiers\nAPI Documentation: https://developer.riotgames.com/apis#tft-match-v1 https://developer.riotgames.com/apis#tft-league-v1\nProcess Overview\nThe raw data is stored as JSON files produced by the API. A single JSON file corresponds to a singular TFT game. All data collection code is included, and the process is fully reproducible if the user has an valid Riot Game API key and account.\nConstraints of the script: I am limiting the region to NA I originally collected a simple random sample of 250 Grandmaster, and Challenger rank players (the two highest level player ranks) and their 25 most recent game data. However, the API has run limitations which caused my code to only pull 422 players. However, since a single game contains 8 players, the script captured 8862 unique player ids which should be sufficently robust.\nAfter some inital evaluation, I wanted to include the next lower rank (Master) to better diversify the playerbase (according to as Challenger + Grandmaster accounted for &lt;1% of the playerbase while Master is 2.2% (source). However, when I ran the script the RIOT API had no data to pull due to the new build coming out.\nUsed github script tft_data_collector.py as the basis for my script (source: )."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Introduction The purpose of this data cleaning page was to take the JSON files created in the data-collecting step, combine and flatten the data into a singular Pandas data frame. Data cleaning was an interative process I needed to add and alter throughout the Project. Once the Pandas dataframe was build, i needed to remove parts of the dataset, and create additional calculated fields that would be used in the EDA, supervised, and unsupervised learning sections.This final data frame was then saved as a csv file for both documentation and later use in the EDA step.\nProcess The raw JSON file consists of multi-level nested dictionaries and lists. To flatten these into a dataframe, I used Python.\nOnce the JSON files had been combined, the shape was (50825, 23). 8662 unique player IDs across 6388 game were captured which is still robust despite my limitations.\nI first looked at the data types. Data types were object, int64, and float 64. Time fields game_length and time_eliminated being float was acceptable as time was measured in seconds.\nMissing data was identified using this code line: dfRaw.notnull().mean() * 100 Which provided how many of a fields data was null.\nMy result was 100% except for the field augments which was 0. As augments had no data, it was removed. This was likely due an error on the data-collection process. I was unable to resolve this due to the Riot API having no data to pull when I reran due to the release of Build 16. The rest of my fields had no null values.\nOutlier Detection and Treatment and Normalization and Scaling were not performed at this time. I wanted my final cleaned dataset to not alter the data that exists. Instead, looking at outliers, normalization, scaling, skew and kurtosis are evaluated in the EDA section, and then performed during the modeling section.\nThe final dataset was saved as csv file for usage in EDA, unsupervised and supervised learning sections."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "References\n\n1. ChatGPT, version-3.5, OpenAI, aug-2023, chat.openai.com.\n\n\n2. ChatGPT, version-4o, OpenAI, may-13, 2024, chat.openai.com.\n\n\n3. ‘Propmpt: Describe what k-mean is‘ ChatGPT, version-4o, OpenAI, may-13, 2024, chat.openai.com.\n\n\n4. The following code cell was generate primarily via GPT-4o, OpenAI, chat.openai.com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing Page",
    "section": "",
    "text": "Title: Statistical Analysis of TeamFight Tactics Top level Gameplay Motivation: My interest in the intersection of data science and the video-game industry inspired this project. In November 2025, Riot Games announced an opening for a Teamfight Tactics (TFT) Insights Analyst internship. After downloading TFT and spending time learning the game, I enjoyed the gameplay and its gameplay mechanics. A high amount of resource managment and long term stragetic planning mixed with in the moment improv due to critical elements such as the aguments received, and champions purchased are RNG. Questiosn like should I buy exp so I can deploy more units or should I keep spending gold to reroll so I can get 2 or 3 star units? Despite the RNG creating uncertainty in outcome, it did not at the total expense of skill.\nThis project uses TFT as a case study for applying statistical and data analytics to video games systems. I hope this work demonstrates how data science can support decision-making in esports analytics—an area that continues to grow in both academic relevance and industry demand."
  },
  {
    "objectID": "index.html#project-introduction",
    "href": "index.html#project-introduction",
    "title": "Landing Page",
    "section": "",
    "text": "Title: Statistical Analysis of TeamFight Tactics Top level Gameplay Motivation: My interest in the intersection of data science and the video-game industry inspired this project. In November 2025, Riot Games announced an opening for a Teamfight Tactics (TFT) Insights Analyst internship. After downloading TFT and spending time learning the game, I enjoyed the gameplay and its gameplay mechanics. A high amount of resource managment and long term stragetic planning mixed with in the moment improv due to critical elements such as the aguments received, and champions purchased are RNG. Questiosn like should I buy exp so I can deploy more units or should I keep spending gold to reroll so I can get 2 or 3 star units? Despite the RNG creating uncertainty in outcome, it did not at the total expense of skill.\nThis project uses TFT as a case study for applying statistical and data analytics to video games systems. I hope this work demonstrates how data science can support decision-making in esports analytics—an area that continues to grow in both academic relevance and industry demand."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Landing Page",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhich variables most strongly influence overall player placement?\nWhich factors are most predictive of finishing in first place?\nWhat are the most popular comps used at top level play?\nWhat comp consistently places the highest?\nWhat champions are seen the most at top level play?"
  },
  {
    "objectID": "index.html#literature-review-1",
    "href": "index.html#literature-review-1",
    "title": "Landing Page",
    "section": "Literature Review 1",
    "text": "Literature Review 1\nTitle: Predicting Teamfight Tactics Results with Machine Learning Techniques. https://researchportal.hkr.se/en/studentTheses/predicting-teamfight-tactics-results-with-machine-learning-techni-5/\nSummary: This undergraduate thesis evaluates whether machine-learning models commonly applied in traditional sports analytics can effectively predict outcomes within esports, using Teamfight Tactics as a test environment. The authors construct predictive models using features such as champions on the board, unit star levels, synergies, and item configurations. The study also highlights structural differences between esports and physical sports, particularly the rapidly shifting rulesets and high-dimensional gameplay data unique to TFT.\nKey Findings:\n\nTFT datasets exhibit high dimensionality, meaning that a large number of interdependent variables must be considered for accurate prediction.\nPredictive performance improved when features captured strategic structure (synergies, items) rather than simple counts of units.\n\nTakeaway: This study demonstrates that TFT outcomes can be partially predicted by features representing team composition and champion quality. Also, the study states of the high dimensionality within the dataset."
  },
  {
    "objectID": "index.html#literature-review-2",
    "href": "index.html#literature-review-2",
    "title": "Landing Page",
    "section": "Literature Review 2",
    "text": "Literature Review 2\nTitle: E-Sports Analytics: A Primer and Resource for Student Research Projects and Lesson Plans https://files.eric.ed.gov/fulltext/EJ1242004.pdf\nSummary: This paper provides an overview of esports analytics as an emerging academic domain. It outlines common analytical techniques used in competitive games—such as classification, clustering, and player behavioral modeling—and discusses the challenges caused by dynamic rulesets and patch updates.\nKey Findings:\n\nTraditional statistical and machine-learning techniques are relevant to esports datasets such as TFT, but must account for sudden changes in gameplay mechanics.\n\nTakeaway: This paper reinforces the value of analyzing TFT using structured statistical methods while acknowledging the complexity introduced by multiple game patches. It supports the inclusion of patch-specific analysis in my research questions"
  },
  {
    "objectID": "index.html#literature-review-3",
    "href": "index.html#literature-review-3",
    "title": "Landing Page",
    "section": "Literature Review 3",
    "text": "Literature Review 3\nTitle: ”Getting Mortdogged”; How high-ranking players experience randomness in Teamfight Tactics (TFT) https://uu.diva-portal.org/smash/get/diva2%3A1805620/FULLTEXT01.pdf\nSummary: This bachelor’s thesis examines how high-level TFT players interpret and respond to the inherent randomness embedded in the game design. Through qualitative interviews, the authors analyze how players conceptualize probability, perceive fairness, and adapt strategies to mitigate shop RNG, augment outcomes, item drops, and combat variance.\nKey Findings:\n\nAutoBattler video games (such as TFT) lack academic research despite high popularity.\nDespite a high degree of randomness, TFT is experienced by high-ranked players as a skill-expressive game; randomness shapes strategic flexibility rather than undermining competitive integrity.\nPlayers often develop heuristics for predicting variance trends, demonstrating that even stochastic elements can be incorporated into consistent strategies.\n\nTakeaway: This thesis shows the competitive depth of TFT and the importance of analyzing concrete, measurable gameplay features—such as champion choices and unit tiers—rather than viewing outcomes as purely luck-driven. Also, top level players can adapt to randomness to create consistent meta strategies such as specific team compositions.\nReferences:"
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Over the past fifteen years, esports has transformed from a niche hobby into a global phenomenon, drawing millions of players, viewers, and investors. Companies such as Capcom host their own competitive torunament cicuit with the winner of the final tournament (Capcom Cup) winning $1.5 million dollars in Dec 2025. As competitive gaming matured, it evolved beyond entertainment into a complex ecosystem shaped by data, strategy, and real-time decision-making.\nReleased in 2019 by Riot Games, Teamfight Tactics (TFT) has became one of the most popular games in the auto-battler genre. TFT challenges players to make optimal strategic decisions and resource management, while constantly adapting to randomness.\nAs TFT’s community and competitive scene expanded, so did the importance of understanding how players make decisions and what factors ultimately determine success. For developers, individual players, esports teams, and marketing strategists, insights into player behavior can guide content updates, balance patches, monetization systems, and engagement strategies.\nThis report explores the underlying factors that influence competitive performance in TFT and demonstrates how data-driven analysis can reveal meaningful patterns in player outcomes."
  },
  {
    "objectID": "report/report.html#introduction",
    "href": "report/report.html#introduction",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Over the past fifteen years, esports has transformed from a niche hobby into a global phenomenon, drawing millions of players, viewers, and investors. Companies such as Capcom host their own competitive torunament cicuit with the winner of the final tournament (Capcom Cup) winning $1.5 million dollars in Dec 2025. As competitive gaming matured, it evolved beyond entertainment into a complex ecosystem shaped by data, strategy, and real-time decision-making.\nReleased in 2019 by Riot Games, Teamfight Tactics (TFT) has became one of the most popular games in the auto-battler genre. TFT challenges players to make optimal strategic decisions and resource management, while constantly adapting to randomness.\nAs TFT’s community and competitive scene expanded, so did the importance of understanding how players make decisions and what factors ultimately determine success. For developers, individual players, esports teams, and marketing strategists, insights into player behavior can guide content updates, balance patches, monetization systems, and engagement strategies.\nThis report explores the underlying factors that influence competitive performance in TFT and demonstrates how data-driven analysis can reveal meaningful patterns in player outcomes."
  },
  {
    "objectID": "report/report.html#purpose",
    "href": "report/report.html#purpose",
    "title": "DSAN-5000: Project",
    "section": "Purpose",
    "text": "Purpose\nThis report aims to identify the most important gameplay factors that determine competitive outcomes in Teamfight Tactics. Specifically, the analysis investigates:\n\nWhich variables most strongly influence overall player placement\nWhich factors are most predictive of finishing in first place.\nWhat are the most popular comps used at top level play?\nWhat comp consistently places the highest?\n\nFor this study, only Grandmaster and Challenge level match data from Version 15.22 was used to ensure that the insights reflect strategic behavior at the highest levels of play rather than general gameplay.\nAlthough the technical models underpinning the analysis are complex, this report is written for a non-technical audience, including marketing teams, esports stakeholders, and business leaders. The goal is to demonstrate how machine-learning insights can support smarter decisions around game balance, player engagement, and competitive design."
  },
  {
    "objectID": "report/report.html#key-findings",
    "href": "report/report.html#key-findings",
    "title": "DSAN-5000: Project",
    "section": "Key Findings",
    "text": "Key Findings\n\nThe most sucessful comp found at top level play lacked a high consistency in the champion selection.\nThis comp consisted of\nThe range of placements for the 11 comps I build was less than 1, indicating a well balanced metagame\nWhile Braum was the popular unit, he did not appear on the best placing or most common comp."
  },
  {
    "objectID": "report/report.html#methodology-overview",
    "href": "report/report.html#methodology-overview",
    "title": "DSAN-5000: Project",
    "section": "Methodology Overview",
    "text": "Methodology Overview\nAll of the data was collected from Riot Games TFT API. The 25 most recent matches were collected from a simple random sample of 422 Grandmaster and Challenger ranked players. For supervised learning, Random Forest Regression and Random Forest Classification were applied, while Hieratchic Clustering was applied for unsupervised learning."
  },
  {
    "objectID": "report/report.html#conclusions",
    "href": "report/report.html#conclusions",
    "title": "DSAN-5000: Project",
    "section": "Conclusions",
    "text": "Conclusions\nThe video game industry has a robust degree of data that can offer insights into gameplay mechanics, player engagement, and business descisions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nGetting the API key operating correctly was an incredibly tedious process. After getting the API key approval, my TFT account wouldnt synch with my Riot Games account preventing me from using the key. This took about 11 hours to resolve.\nI originally planned to obtain a simple random sample of 250 Grandmaster, and Challenger rank players (the two highest level player ranks) and their 25 most recent game data. However, the API has run limitations which caused my code to only pull 422 players.\nAfter some inital evaluation, I wanted to include the next lower rank (Master) to better diversify the playerbase (Challenger + Grandmaster accounted for &lt;1% of the playerbase while Master is 2.2%).\nHowever, when I ran the script the RIOT API had no data to pull due to the new build coming out.\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\nThere are many areas for future work that would be intresting. Comparing game versions to see if balance patches had achieved the deseriable outcome.\n\n\n\n\n\nBased on my personal gameplay experience, my expectations for most relevant variables to determine match placement would be game length, damage dealt. These were proven correct when I performed\nOutside of those, I had thought the presecne of 3 star units would nearly guarentee upper placement.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Getting the API key operating correctly was an incredibly tedious process. After getting the API key approval, my TFT account wouldnt synch with my Riot Games account preventing me from using the key. This took about 11 hours to resolve.\nI originally planned to obtain a simple random sample of 250 Grandmaster, and Challenger rank players (the two highest level player ranks) and their 25 most recent game data. However, the API has run limitations which caused my code to only pull 422 players.\nAfter some inital evaluation, I wanted to include the next lower rank (Master) to better diversify the playerbase (Challenger + Grandmaster accounted for &lt;1% of the playerbase while Master is 2.2%).\nHowever, when I ran the script the RIOT API had no data to pull due to the new build coming out.\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\nThere are many areas for future work that would be intresting. Comparing game versions to see if balance patches had achieved the deseriable outcome."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Based on my personal gameplay experience, my expectations for most relevant variables to determine match placement would be game length, damage dealt. These were proven correct when I performed\nOutside of those, I had thought the presecne of 3 star units would nearly guarentee upper placement."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nAll of the player match data was collected using Riot Games’ official Teamfight Tactics API. Identifiers of players rank (Grandmaster, and Challenger) from North America (NA1) were preset. From this population, a simple random sample of 250 players of each rank were selected. The 25 most recent games played by these players were then selected. This data was then stored as raw JSON files to latter be processed in the data processing step."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "#Introduction\nThis analyses performs basic EDA to provide a high level picture of the cleaned TFT dataset. First, I am only looking at Version 15.22. Different game versions have balance patches which can affect gameplay in major ways.\nThis EDA begins with a basic info and descriptive statistics of the int and float fields. Then a histogram was made to get a general view of the distributions.The results of this showed some high degree of bias so the skew and kurtosis was taken. num_3star, num_cost3, 2.258098, 3.439884 had skew &gt; 1. For kurtosis, game_length = 29 and gold_left = 12. Based on these results, gold_left was an incredibly biased varaible i needed to consider when performing supervised and unsupervised learning. I also made a correlation matrix, and pairplot to see if any notable relationships among data was formed.\nFinally, to get a sense of unit popularity and team comps, I created a bar chart of the top 10 most used champions and a correlation heatmap of units. The most popular units were Braum, KSante, Udyr, Swain, Zyra, JarvanIV, Leona, LeeSin, Vi, and Janna.\nAs for the heatmap results, there were some notable correlations in Braum + Swain, Braum + Zyra, Janna + JarvanIV. When I try to determine comps, I wouldlike to see if these champions are frequently seen or the pairings I found are present."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "11-01-2025\n\nDecided topic based on internship application post.\n\n11-03-2025\n\nConducted intial research/documentation on auto battler and Teamfight tactics\n\n11-07-2025\n\nAdded Sample Code for Riot API data extraction\n\n11-12-2025\n\nCreated inital Riot API data extraction code\n\n11-14-2025\n\nBegan writing instructions for data-cleaning and data-collection. And edited documentation.\n\n11-22-2024\n\nAPI scrapping didnt work. Had to troubleshoot extensivlety. Got it to work and redid API script. API script ran sucessfully.\n\n11-23-2025\n\nEvaluated JSON scripts from API code. Then wrote code that would combine into singlar file for data processing.\n\n11-30-2025\n\nTested and ran script to combine JSON into a singular csv file which will be used for data processing. Updated documentation page\n\n12-02-2024 * Created unsupervised clutering using HDBSCAN.\n12-03-2025 * Redid unsupervised learning. Hierachitc Clustering was superior method."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Introduction:",
    "section": "",
    "text": "Introduction:\nThis project explores unsupervised learning techniques to identify common meta compositions (“comps”) used by top-ranked players in Teamfight Tactics (TFT), Set 15. The primary research questions guiding this analysis are:\nWhat team composition is the most frequently used by top-level TFT players? Which composition places the highest at top-level play?\nIn TFT, champions have unique abilities and share traits that activate synergies when fielded together. For example, Ahri and Jinx share the Star Guardian trait, granting increased bonuses when multiple Star Guardians appear in the team. Understanding which champion combinations appear consistently in successful boards may reveal the meta trends that drive competitive play.\n\n\nMethods\nI had used my cleaned dataset to create a new dataset consisting of 32,455 rows and 63 champion indicator columns, where each column represented the frequency or presence of a champion in the player’s final board. Because analyzing every individual game state introduces high redundancy, duplicate rows were removed, producing a dataset of 10,757 unique compositions.\nThis reduction is expected because high-level players tend to converge on optimal meta comps despite the game’s inherent randomness (RNG)\nInitial Attempts Using HDBSCAN\nMy initial attemp was to use use HDBSCAN, which is well-suited for clusters of varying densities. However, it was not producing ideal results. As the data is purely categorical, Euclidean distance is unsuitable. Switching to Jaccard distance produced unstable clusters with poor separation.\nDue to these limitations, I had decided to apply hierarchial clustering instead. Like HDBSCAN, it was initially run using Jaccard distance, but the number of clusters exceeded 100, making interpretation difficult. To address this, Principal Component Analysis (PCA) was applied prior to clustering. PCA was applied with n_components = 25 and the retained components explained 82.6% of the variance. After PCA was applied, clustering transitioned to using Euclidean distance.\nAfter creating a dendrogram, a threshold parameter (t) was evaluated to determine the cut point in the dendrogram. This would determine the number of clusters. The results are shown below:\nt = 2 → clusters = 228 t = 3 → clusters = 11 t = 4 → clusters = 1 t = 5 → clusters = 1\nBased on cluster separation and number of clusters, t = 3 was selected, producing 11 meaningful clusters representing meta team compositions.\n#Methods Hierarchic Clustering: - A clustering algorithm.\nHDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) - HDBSCAN is a clustering algorithm build upon DBSCAN. It can dynamically adapt adjust to different densities and form clusters within the data. It is best used for datasets with complex structures or varying densities because it creates a hierarchical tree of clusters that enable users to examine the data at different levels of granularity. Parameters that can be adjusted by the ende user are: The minimum number of points to form a cluster. The minimum number of samples in a neighborhood for a point to be considered a core point.\nPrincipal Component Analysis (PCA): -\nTSNE -\n#Results Section: Below is a table showing the cluster and the number of points in each cluster.\n3 1680 6 1505 5 1363 10 960 1 955 4 852 2 819 9 802 11 744 7 566 8 511\nClusters 3,6, and 5 are the largest.\nBelow are the 11 clusters produced. The values represent frequency of champion appearance within the cluster. A higher value, indicates if they are a staple,secondary, or niche champion pick for each team comp.\n===== COMP 1 ===== Poppy: 0.95 Jinx: 0.95 Neeko: 0.81 Rell: 0.75 Kobuko: 0.72 KSante: 0.68 Xayah: 0.52 Seraphine: 0.48 Varus: 0.47 Braum: 0.46\n===== COMP 2 ===== Garen: 0.95 Rakan: 0.90 Ezreal: 0.88 Leona: 0.84 Yuumi: 0.83 Jayce: 0.57 Katarina: 0.53 Ryze: 0.31 Malzahar: 0.29 KSante: 0.29\n===== COMP 3 ===== Shen: 0.89 Leona: 0.85 Rell: 0.84 Swain: 0.83 XinZhao: 0.81 Braum: 0.66 Garen: 0.43 Seraphine: 0.38 TwistedFate: 0.37 Zyra: 0.31\n===== COMP 4 ===== Samira: 0.93 Sett: 0.92 Volibear: 0.77 Naafiri: 0.73 XinZhao: 0.72 Lux: 0.54 Gwen: 0.48 Braum: 0.46 Viego: 0.40 Shen: 0.38\n===== COMP 5 ===== DrMundo: 0.93 Udyr: 0.91 Sett: 0.87 Naafiri: 0.80 Vi: 0.71 LeeSin: 0.71 Braum: 0.54 TwistedFate: 0.53 Zyra: 0.45 Aatrox: 0.44\n===== COMP 6 ===== Swain: 0.91 Janna: 0.90 Ashe: 0.85 Vi: 0.77 Zyra: 0.71 KSante: 0.69 JarvanIV: 0.67 Braum: 0.55 Udyr: 0.53 LeeSin: 0.47\n===== COMP 7 ===== Janna: 0.83 Malphite: 0.81 Sivir: 0.60 Ziggs: 0.57 Shen: 0.57 Ryze: 0.46 JarvanIV: 0.44 Neeko: 0.41 KSante: 0.34 Kennen: 0.31\n===== COMP 8 ===== Karma: 0.97 Lucian: 0.94 JarvanIV: 0.90 Ryze: 0.66 Swain: 0.51 Aatrox: 0.49 Senna: 0.46 Gwen: 0.44 Lux: 0.43 Gangplank: 0.41\n===== COMP 9 ===== Kayle: 0.82 Udyr: 0.79 Zac: 0.72 Jhin: 0.62 Aatrox: 0.60 Varus: 0.54 KSante: 0.54 LeeSin: 0.40 Malzahar: 0.40 Sett: 0.31\n===== COMP 10 ===== Zac: 0.64 Darius: 0.64 Malzahar: 0.64 Kobuko: 0.58 Poppy: 0.50 Jayce: 0.46 Aatrox: 0.37 Rammus: 0.32 Seraphine: 0.28 KSante: 0.21\n===== COMP 11 ===== Aatrox: 0.75 Ryze: 0.71 Udyr: 0.65 JarvanIV: 0.62 Akali: 0.56 Darius: 0.55 Senna: 0.49 Kennen: 0.49 Kobuko: 0.44 KSante: 0.41\nAfter looking at these comps, I then found the average place of each cluster.\ncluster 10 4.206250 4 4.497653 8 4.524462 11 4.526882 1 4.689005 2 4.739927 9 4.754364 3 4.783929 5 4.947175 6 5.016611 7 5.169611\n\nConclusions:\nThe most popular competitve comp was Comp 3, a Shen–Leona Vanguard team.\n===== COMP 3 ===== Shen: 0.89 Leona: 0.85 Rell: 0.84 Swain: 0.83 XinZhao: 0.81 Braum: 0.66 Garen: 0.43 Seraphine: 0.38 TwistedFate: 0.37 Zyra: 0.31\nShen, Leona, Rell, Swain, and Xin Zhao form a stable and highly repeatable core, each appearing in over 80% of compositions in this cluster. Braum serves as a common secondary unit, while Garen, Seraphine, Twisted Fate, and Zyra act as niche and/or situational inclusions.\nBecause this cluster is the largest, it represents the most frequently played comp among top-ranked players. However, despite its popularity, Comp 3 had only the fourth-best average placement, indicating that frequency does not necessarily equate to performance.\nThe highest placing was comp 10: ===== COMP 10 ===== Zac: 0.64 Darius: 0.64 Malzahar: 0.64 Kobuko: 0.58 Poppy: 0.50 Jayce: 0.46 Aatrox: 0.37 Rammus: 0.32 Seraphine: 0.28 KSante: 0.21\nThis comp is a tank/bruiser-oriented lineup anchored by Zac, Darius, Malzahar, and Kobuko. Interestingly, Comp 10 shows comparatively low internal cohesion—its most frequent champions appear only 64% of the time. The lack of a clearly defined, consistent core likely contributes to its lower pick rate, even though it performs extremely well when executed.\nComparing the most popular and best performing comp leads to some results of the metagame. There is almost no overlap between the top-performing and the most popular comps, with Seraphine being the only shared champion—and only as a low-frequency flex unit in both. Also, the placement range across all clusters is narrow (less than one position), indicating a highly balanced metagame at the top level of competitive play."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "All of the data for this project was collected directly from the official Riot Games Teamfight Tactics (TFT) API, which is a public API maintained by Riot Games. It provides real-time and historical game data for TFT players, matches, and ranked ladders.\nData Tables Used:\nLeague endpoints: retrieves Master, Grandmaster, and Challenger ranked players Match endpoints: retrieves match history and full match JSON data PUUID lookup: maps players to their unique global identifiers\nAPI Documentation: https://developer.riotgames.com/apis#tft-match-v1 https://developer.riotgames.com/apis#tft-league-v1\nProcess Overview\nThe raw data is stored as JSON files produced by the API. A single JSON file corresponds to a singular TFT game. All data collection code is included, and the process is fully reproducible if the user has an valid Riot Game API key and account.\nConstraints of the script: I am limiting the region to NA I originally collected a simple random sample of 250 Grandmaster, and Challenger rank players (the two highest level player ranks) and their 25 most recent game data. However, the API has run limitations which caused my code to only pull 422 players. However, since a single game contains 8 players, the script captured 8862 unique player ids which should be sufficently robust.\nAfter some inital evaluation, I wanted to include the next lower rank (Master) to better diversify the playerbase (according to as Challenger + Grandmaster accounted for &lt;1% of the playerbase while Master is 2.2% (source). However, when I ran the script the RIOT API had no data to pull due to the new build coming out.\nUsed github script tft_data_collector.py as the basis for my script (source: )."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-from-riot-games-api",
    "href": "technical-details/data-collection/main.html#data-collection-from-riot-games-api",
    "title": "Data Collection",
    "section": "Data Collection from Riot Games API",
    "text": "Data Collection from Riot Games API\nPurpose: In the following code, we be collecting data from the Riot Games API using Python.\nConstraints of the script: I am limiting the reigion is to NA I originally wanted a simple random sample of 250 Grandmaster, and Challenger rank players (the two highest level player ranks) and their 25 most recent game data. However, the API has run limitations which caused my code to only pull 422 players.\nAfter some inital evaluation, I wanted to include the next lower rank (Master) to better diversify the playerbase (as Challenger + Grandmaster accounted for &lt;1% of the playerbase while Master is 2.2%. However, when I ran the script the RIOT API had no data to pull due to the new season coming out. When this happens, the ranks are reset preventing me from pulling the Master rank players.\nThis script is still configured to pull Master player data. This scripts only outputs will be the JSON files pulled from the API. All files are located in data/raw-data\n\n#Import python packges script uses\nimport os\nimport time\nimport json\nimport random\nimport logging\nimport requests\nfrom pathlib import Path\nimport pandas as pd\n\n\n#NOTE: This code block was made with Chat-GPT 5.0\n\n#This is a test to see if API key works\nAPI_KEY = \"PLACEHOLDER\" #Note: when published make PLACEHODLER\n\n#Fully play a game of TFT. At the end, there is a match ID.\nmatch_id = \"NA1_5419735432\"\n\nurl = f\"https://americas.api.riotgames.com/tft/match/v1/matches/{match_id}\"\nres = requests.get(url, params={\"api_key\": API_KEY})\n\n#If STATUS = 200, then API key is good.\nprint(\"STATUS:\", res.status_code)\nprint(res.text[:3000])\n\nSTATUS: 200\n{\"metadata\":{\"data_version\":\"6\",\"match_id\":\"NA1_5419735432\",\"participants\":[\"z7tXLLhq0AA5E2G4Jdzq88KOlSkOndkZ56mWJnwqAzT5IV_6EOYhU1_ZySsxZtyudA53apw2vs_p5Q\"]},\"info\":{\"endOfGameResult\":\"GameComplete\",\"gameCreation\":1763881823000,\"gameId\":5419735432,\"game_datetime\":1763883928853,\"game_length\":2086.93115234375,\"game_version\":\"Linux Version 15.23.728.3286 (Nov 21 2025/16:26:55) [PUBLIC] &lt;Releases/15.23&gt;\",\"mapId\":22,\"participants\":[{\"companion\":{\"content_ID\":\"5897ad9f-4665-4372-8f3e-6c878adb8918\",\"item_ID\":1,\"skin_ID\":1,\"species\":\"PetTFTAvatar\"},\"gold_left\":38,\"last_round\":37,\"level\":9,\"missions\":{\"PlayerScore2\":207},\"placement\":1,\"players_eliminated\":3,\"puuid\":\"z7tXLLhq0AA5E2G4Jdzq88KOlSkOndkZ56mWJnwqAzT5IV_6EOYhU1_ZySsxZtyudA53apw2vs_p5Q\",\"riotIdGameName\":\"vornelix\",\"riotIdTagline\":\"819\",\"time_eliminated\":2078.66748046875,\"total_damage_to_players\":267,\"traits\":[{\"name\":\"TFT15_Bastion\",\"num_units\":2,\"style\":1,\"tier_current\":1,\"tier_total\":3},{\"name\":\"TFT15_Captain\",\"num_units\":1,\"style\":3,\"tier_current\":1,\"tier_total\":1},{\"name\":\"TFT15_Edgelord\",\"num_units\":2,\"style\":1,\"tier_current\":1,\"tier_total\":3},{\"name\":\"TFT15_ElTigre\",\"num_units\":1,\"style\":3,\"tier_current\":1,\"tier_total\":1},{\"name\":\"TFT15_GemForce\",\"num_units\":2,\"style\":0,\"tier_current\":0,\"tier_total\":4},{\"name\":\"TFT15_Luchador\",\"num_units\":2,\"style\":1,\"tier_current\":1,\"tier_total\":2},{\"name\":\"TFT15_Prodigy\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":4},{\"name\":\"TFT15_Protector\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":3},{\"name\":\"TFT15_Rosemother\",\"num_units\":1,\"style\":3,\"tier_current\":1,\"tier_total\":1},{\"name\":\"TFT15_SoulFighter\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":5},{\"name\":\"TFT15_Spellslinger\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":3},{\"name\":\"TFT15_StarGuardian\",\"num_units\":3,\"style\":2,\"tier_current\":2,\"tier_total\":12},{\"name\":\"TFT15_Strategist\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":4},{\"name\":\"TFT15_TheCrew\",\"num_units\":1,\"style\":0,\"tier_current\":0,\"tier_total\":7}],\"units\":[{\"character_id\":\"TFT15_Rell\",\"itemNames\":[\"TFT_Item_SpectralGauntlet\",\"TFT_Item_TearOfTheGoddess\"],\"name\":\"\",\"rarity\":0,\"tier\":3},{\"character_id\":\"TFT15_Janna\",\"itemNames\":[\"TFT_Item_FrozenHeart\"],\"name\":\"\",\"rarity\":1,\"tier\":2},{\"character_id\":\"TFT15_Ahri\",\"itemNames\":[],\"name\":\"\",\"rarity\":2,\"tier\":1},{\"character_id\":\"TFT15_Ahri\",\"itemNames\":[\"TFT_Item_InfinityEdge\",\"TFT_Item_NeedlesslyLargeRod\",\"TFT_Item_ForceOfNature\"],\"name\":\"\",\"rarity\":2,\"tier\":2},{\"character_id\":\"TFT15_Samira\",\"itemNames\":[],\"name\":\"\",\"rarity\":4,\"tier\":2},{\"character_id\":\"TFT15_Volibear\",\"itemNames\":[],\"name\":\"\",\"rarity\":4,\"tier\":2},{\"character_id\":\"TFT15_Braum\",\"itemNames\":[\"TFT_Item_BFSword\",\"TFT_Item_GuinsoosRageblade\",\"TFT_Item_RunaansHurricane\"],\"name\":\"\",\"rarity\":6,\"tier\":2},{\"character_id\":\"TFT15_Seraphine\",\"itemNames\":[\"TFT_Item_JeweledGauntlet\",\"TFT_Item_Crownguard\"],\"name\":\"\",\"rarity\":6,\"tier\":1},{\"character_id\":\"TFT15_TwistedFate\",\"itemNames\":[],\"name\":\"\",\"rarit"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "The goal of this project is to identify the most important factors that determine player placement in high-level Teamfight Tactics (TFT) games. Two related research questions were explored:\n\nWhich variables have the greatest impact on overall placement among top-ranked players?\nWhich variables are most predictive of finishing in first place?\n\nTo answer these questions, two machine learning models were used:\n\nRandom Forest Regressor — to predict a player’s numerical placement\nRandom Forest Classifier — to predict whether a player finished first (binary outcome)\n\nRandom Forest models were chosen because they handle nonlinear relationships, do not require normalization or scaling, and naturally provide feature importance rankings. This makes them well-suited for gameplay-derived variables that differ widely in scale and distribution.\nThe dataset included various gameplay metrics such as gold remaining, number of items used, total champions fielded, and distribution of champion star levels and costs. These features were used to train both models after performing standard train–test splits.\nWhat are the most significant factors that determine placement at top level play? What are the most significant factors that determine first place?\nTo best awnser this, I used Random Forest Regressor. My choices was due to the fact Random Forest Regressor does not require normalization, feature scaling, or linearity assumptions. For determining first place, I decided on Random Forest Classifer given that the outcome Im trying to predict is a binary categorical variable."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction",
    "href": "technical-details/supervised-learning/main.html#introduction",
    "title": "Supervised Learning",
    "section": "",
    "text": "The goal of this project is to identify the most important factors that determine player placement in high-level Teamfight Tactics (TFT) games. Two related research questions were explored:\n\nWhich variables have the greatest impact on overall placement among top-ranked players?\nWhich variables are most predictive of finishing in first place?\n\nTo answer these questions, two machine learning models were used:\n\nRandom Forest Regressor — to predict a player’s numerical placement\nRandom Forest Classifier — to predict whether a player finished first (binary outcome)\n\nRandom Forest models were chosen because they handle nonlinear relationships, do not require normalization or scaling, and naturally provide feature importance rankings. This makes them well-suited for gameplay-derived variables that differ widely in scale and distribution.\nThe dataset included various gameplay metrics such as gold remaining, number of items used, total champions fielded, and distribution of champion star levels and costs. These features were used to train both models after performing standard train–test splits.\nWhat are the most significant factors that determine placement at top level play? What are the most significant factors that determine first place?\nTo best awnser this, I used Random Forest Regressor. My choices was due to the fact Random Forest Regressor does not require normalization, feature scaling, or linearity assumptions. For determining first place, I decided on Random Forest Classifer given that the outcome Im trying to predict is a binary categorical variable."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#methods",
    "href": "technical-details/supervised-learning/main.html#methods",
    "title": "Supervised Learning",
    "section": "Methods",
    "text": "Methods\nRandom Forest Regressor: - A Random Forest Regressor is an ensemble learning method that builds many decision trees and averages their predictions. It performs well on nonlinear data and identifies which features contribute most to prediction accuracy.\nRandom Forest Classifier: - A Random Forest Classifier uses the same ensemble strategy but outputs categorical predictions. In this case, the model predicted whether a player achieved first place (1) or not (0).\nFeature Definitions The following gameplay features were used:\ngold_left — Gold remaining when the game ends total_items — Number of completed or component items used num_units — Total number of champions deployed num_1star / 2star / 3star — Count of champions by star level num_cost1 / 2 / 3 / 4 / 5 — Count of champions by shop cost\nSeveral variables were intentionally omitted, including time_eliminated, total_damage, and game_length, because they strongly correlate with placement and would cause leakage, making the model trivially predict outcomes.\nModel Training\nThe dataset was split using train_test_split. For the classifier predicting first place, the split used: stratify = y1 Stratify was applied because first place finishes accounted for 12.5% of the total dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "Results",
    "text": "Results\nRandom Forest Regressor Initial model performance: MAE = 0.548 MSE = 0.536 RMSE = 0.732\nAfter the train–test split:\nMAE = 1.168 MSE = 2.261 RMSE = 1.504\nDespite omitted variable bias, these values indicate reasonably strong performance given the inherent randomness of TFT.\nImportant Predictors:\ntotal_items ≈ 0.30 num_2star gold_left num_units num_3star\nThe most dominant factor was items.\nRandom Forest Classifier\nBelow are metrics of validity:\nAccuracy = 0.880 Precision = 0.531 Recall = 0.386 F1 = 0.447 ROC-AUC = 0.878\nAccuracy and ROC-AUC are strong, but Precision, Recall, and F1 are relatively weak, likely due to the rarity of first-place finishes and the difficulty of modeling endgame volatility.\nThe most important factors were: gold_left ≈ 0.25 total_items num_units num_2star"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusions",
    "href": "technical-details/supervised-learning/main.html#conclusions",
    "title": "Supervised Learning",
    "section": "Conclusions:",
    "text": "Conclusions:\nAcross both models, four features consistently emerged as the most important:\nItems Gold left over Number of 2-star champions Total units deployed\nInterestingly, the total number of champions was less predictive than itemization and gold economy. This suggests that TFT success at top level play depends more on quality of units (upgrades + items) rather than simply quantity of units."
  }
]